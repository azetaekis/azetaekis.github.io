{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Instalaci\u00f3n de Openstack Esto es una gu\u00eda para instalar un entorno de Openstack que se asemeja al siguiente esquema: La gu\u00eda est\u00e1 pensada para un entorno de alta disponibilidad y para ello iremos paso a paso en nueve apartados","title":"Home"},{"location":"#instalacion-de-openstack","text":"Esto es una gu\u00eda para instalar un entorno de Openstack que se asemeja al siguiente esquema: La gu\u00eda est\u00e1 pensada para un entorno de alta disponibilidad y para ello iremos paso a paso en nueve apartados","title":"Instalaci\u00f3n de Openstack"},{"location":"openstack/01-keystone/","text":"1. Keystone 1.0 Teor\u00eda Fernet 1.1 Instalaci\u00f3n controllers Instalamos los paquetes para Keystone: sudo apt install keystone memcached 1.2 Base de datos controllers Creamos la base de datos: sudo mysql CREATE DATABASE keystone; GRANT ALL PRIVILEGES ON keystone.* TO 'keystoneuser'@'localhost' IDENTIFIED BY 'keystonepass'; GRANT ALL PRIVILEGES ON keystone.* TO 'keystoneuser'@'%' IDENTIFIED BY 'keystonepass'; Editamos el fichero /etc/keystone/keystone.conf : [default] bind_host = 172.16.100.1x public_bind_host = 192.168.111.1x admin_bind_host = 172.16.100.1x [database] #connection = sqlite:////var/lib/keystone/keystone.db connection = mysql+pymysql://keystoneuser:keystonepass@controller/keystone [cache] enabled = true backend = dogpile.cache.memcached memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 Reiniciamos Apache : service apache2 restart Controller01 Poblamos la base de datos: sudo su -s /bin/sh -c \"keystone-manage db_sync\" keystone 1.3 Inicializaci\u00f3n controller01 Inicializamos los repositorios de claves Fernet: sudo keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone sudo keystone-manage credential_setup --keystone-user keystone --keystone-group keystone Para que los nodos de Keystone funcionen correctamente, el repositorio de claves Fernet tiene que estar replicado. Ahora rotamos las claves y las sincronizamos en el resto de nodos keystone-manage fernet_rotate --keystone-user keystone --keystone-group keystone rsync -a -v /etc/keystone/fernet-keys/ controller02:/etc/keystone/fernet-keys/ Y a\u00f1adimos la sincronizaci\u00f3n al cron /etc/cron.d/rsync_fernet : 0 * * * * root /usr/bin/rsync -a -v /etc/keystone/fernet-keys/ controller02:/etc/keystone/fernet-keys/ >/dev/null 2>&1 NOTA: public-api.hispavistaroot.local tiene que resolver a la ip virtual p\u00fablica Inicializamos los endpoints de Keystone: sudo keystone-manage bootstrap --bootstrap-password admin \\ --bootstrap-public-url http://public-api.hispavistaroot.local:5000/v3/ \\ --bootstrap-internal-url http://controller:5000/v3/ \\ --bootstrap-admin-url http://controller:35357/v3/ \\ --bootstrap-region-id SanSebastian Exportamos las siguientes variables de entorno: export OS_USERNAME=admin export OS_PASSWORD=admin export OS_PROJECT_NAME=admin export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_DOMAIN_NAME=Default export OS_AUTH_URL=http://controller:35357/v3 export OS_IDENTITY_API_VERSION=3 1.4 Proyectos, roles y usuarios controller01 Creamos los proyectos service y hispavista : openstack project create --domain default --description \"Service Project\" service openstack project create --domain default --description \"Hispavista Project\" hispavista Creamos el usuario hvsistemas : openstack user create --domain default --password hvsistemas hvsistemas Creamos el rol user : openstack role create user A\u00f1adimos al usuario hvsistemas en el proyecto hispavista con rol user : openstack role add --project hispavista --user hvsistemas user Eliminamos las variables de entorno creadas anteriormente: unset OS_USERNAME unset OS_PASSWORD unset OS_PROJECT_NAME unset OS_USER_DOMAIN_NAME unset OS_PROJECT_DOMAIN_NAME unset OS_AUTH_URL unset OS_IDENTITY_API_VERSION Probamos a pedir un token con el usuario admin : openstack \\ --os-username admin \\ --os-project-name admin \\ --os-user-domain-name Default \\ --os-project-domain-name Default \\ --os-auth-url http://controller:35357/v3 \\ --os-identity-api-version 3 \\ token issue Probamos a pedir un token con el usuario hvsistemas : openstack \\ --os-username hvsistemas \\ --os-project-name hispavista \\ --os-user-domain-name Default \\ --os-project-domain-name Default \\ --os-auth-url http://public-api.hispavistaroot.local:5000/v3 \\ --os-identity-api-version 3 \\ token issue 1.5 Apache controller01 Keystone se sirve con mod_wsgi de Apache. Se puede ver la configuraci\u00f3n en /etc/apache2/sites-enabled/keystone.conf Si queremos reiniciar Keystone, debemos reiniciar el servicio apache2 1.6 Ficheros de acceso controller01 Por facilidad, vamos a crear un fichero en /home/hvsistemas/adminrc con el contenido: export OS_USERNAME=admin export OS_PASSWORD=admin export OS_PROJECT_NAME=admin export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_DOMAIN_NAME=Default export OS_AUTH_URL=http://controller:35357/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 Por facilidad, vamos a crear un fichero en /home/hvsistemas/hvsistemasrc con el contenido: export OS_USERNAME=hvsistemas export OS_PASSWORD=hvsistemas export OS_PROJECT_NAME=hispavista export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_DOMAIN_NAME=Default export OS_AUTH_URL=http://public-api.hispavistaroot.local:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 Ahora podemos cargar las variables de entorno con estos ficheros: source adminrc openstack token issue source hvsistemasrc openstack token issue","title":"1. - Keystone"},{"location":"openstack/01-keystone/#1-keystone","text":"","title":"1. Keystone"},{"location":"openstack/01-keystone/#10-teoria","text":"Fernet","title":"1.0 Teor\u00eda"},{"location":"openstack/01-keystone/#11-instalacion","text":"controllers Instalamos los paquetes para Keystone: sudo apt install keystone memcached","title":"1.1 Instalaci\u00f3n"},{"location":"openstack/01-keystone/#12-base-de-datos","text":"controllers Creamos la base de datos: sudo mysql CREATE DATABASE keystone; GRANT ALL PRIVILEGES ON keystone.* TO 'keystoneuser'@'localhost' IDENTIFIED BY 'keystonepass'; GRANT ALL PRIVILEGES ON keystone.* TO 'keystoneuser'@'%' IDENTIFIED BY 'keystonepass'; Editamos el fichero /etc/keystone/keystone.conf : [default] bind_host = 172.16.100.1x public_bind_host = 192.168.111.1x admin_bind_host = 172.16.100.1x [database] #connection = sqlite:////var/lib/keystone/keystone.db connection = mysql+pymysql://keystoneuser:keystonepass@controller/keystone [cache] enabled = true backend = dogpile.cache.memcached memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 Reiniciamos Apache : service apache2 restart Controller01 Poblamos la base de datos: sudo su -s /bin/sh -c \"keystone-manage db_sync\" keystone","title":"1.2 Base de datos"},{"location":"openstack/01-keystone/#13-inicializacion","text":"controller01 Inicializamos los repositorios de claves Fernet: sudo keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone sudo keystone-manage credential_setup --keystone-user keystone --keystone-group keystone Para que los nodos de Keystone funcionen correctamente, el repositorio de claves Fernet tiene que estar replicado. Ahora rotamos las claves y las sincronizamos en el resto de nodos keystone-manage fernet_rotate --keystone-user keystone --keystone-group keystone rsync -a -v /etc/keystone/fernet-keys/ controller02:/etc/keystone/fernet-keys/ Y a\u00f1adimos la sincronizaci\u00f3n al cron /etc/cron.d/rsync_fernet : 0 * * * * root /usr/bin/rsync -a -v /etc/keystone/fernet-keys/ controller02:/etc/keystone/fernet-keys/ >/dev/null 2>&1 NOTA: public-api.hispavistaroot.local tiene que resolver a la ip virtual p\u00fablica Inicializamos los endpoints de Keystone: sudo keystone-manage bootstrap --bootstrap-password admin \\ --bootstrap-public-url http://public-api.hispavistaroot.local:5000/v3/ \\ --bootstrap-internal-url http://controller:5000/v3/ \\ --bootstrap-admin-url http://controller:35357/v3/ \\ --bootstrap-region-id SanSebastian Exportamos las siguientes variables de entorno: export OS_USERNAME=admin export OS_PASSWORD=admin export OS_PROJECT_NAME=admin export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_DOMAIN_NAME=Default export OS_AUTH_URL=http://controller:35357/v3 export OS_IDENTITY_API_VERSION=3","title":"1.3 Inicializaci\u00f3n"},{"location":"openstack/01-keystone/#14-proyectos-roles-y-usuarios","text":"controller01 Creamos los proyectos service y hispavista : openstack project create --domain default --description \"Service Project\" service openstack project create --domain default --description \"Hispavista Project\" hispavista Creamos el usuario hvsistemas : openstack user create --domain default --password hvsistemas hvsistemas Creamos el rol user : openstack role create user A\u00f1adimos al usuario hvsistemas en el proyecto hispavista con rol user : openstack role add --project hispavista --user hvsistemas user Eliminamos las variables de entorno creadas anteriormente: unset OS_USERNAME unset OS_PASSWORD unset OS_PROJECT_NAME unset OS_USER_DOMAIN_NAME unset OS_PROJECT_DOMAIN_NAME unset OS_AUTH_URL unset OS_IDENTITY_API_VERSION Probamos a pedir un token con el usuario admin : openstack \\ --os-username admin \\ --os-project-name admin \\ --os-user-domain-name Default \\ --os-project-domain-name Default \\ --os-auth-url http://controller:35357/v3 \\ --os-identity-api-version 3 \\ token issue Probamos a pedir un token con el usuario hvsistemas : openstack \\ --os-username hvsistemas \\ --os-project-name hispavista \\ --os-user-domain-name Default \\ --os-project-domain-name Default \\ --os-auth-url http://public-api.hispavistaroot.local:5000/v3 \\ --os-identity-api-version 3 \\ token issue","title":"1.4 Proyectos, roles y usuarios"},{"location":"openstack/01-keystone/#15-apache","text":"controller01 Keystone se sirve con mod_wsgi de Apache. Se puede ver la configuraci\u00f3n en /etc/apache2/sites-enabled/keystone.conf Si queremos reiniciar Keystone, debemos reiniciar el servicio apache2","title":"1.5 Apache"},{"location":"openstack/01-keystone/#16-ficheros-de-acceso","text":"controller01 Por facilidad, vamos a crear un fichero en /home/hvsistemas/adminrc con el contenido: export OS_USERNAME=admin export OS_PASSWORD=admin export OS_PROJECT_NAME=admin export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_DOMAIN_NAME=Default export OS_AUTH_URL=http://controller:35357/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 Por facilidad, vamos a crear un fichero en /home/hvsistemas/hvsistemasrc con el contenido: export OS_USERNAME=hvsistemas export OS_PASSWORD=hvsistemas export OS_PROJECT_NAME=hispavista export OS_USER_DOMAIN_NAME=Default export OS_PROJECT_DOMAIN_NAME=Default export OS_AUTH_URL=http://public-api.hispavistaroot.local:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 Ahora podemos cargar las variables de entorno con estos ficheros: source adminrc openstack token issue source hvsistemasrc openstack token issue","title":"1.6 Ficheros de acceso"},{"location":"openstack/02-glance/","text":"2. Glance 2.1 Instalaci\u00f3n controllers NOTA: a d\u00eda 08/02/2018 no hemos podido implementar la v3 de la api de Cinder en Horizon, no la soporta. Instalamos los paquetes para Glance: sudo apt install glance 2.2 Base de datos controller01 Creamos la base de datos: sudo mysql -uroot -p CREATE DATABASE glance; GRANT ALL PRIVILEGES ON glance.* TO 'glanceuser'@'localhost' IDENTIFIED BY 'glancepass'; GRANT ALL PRIVILEGES ON glance.* TO 'glanceuser'@'%' IDENTIFIED BY 'glancepass'; Controllers Editamos el fichero /etc/glance/glance-api.conf : [database] #connection = <None> connection = mysql+pymysql://glanceuser:glancepass@controller/glance Controller01 Poblamos la base de datos: sudo su -s /bin/sh -c \"glance-manage db_sync\" glance 2.3 Servicio, usuario y endpoints controller01 Creamos el usuario glance y lo a\u00f1adimos al proyecto service con el rol admin : source adminrc openstack user create --domain default --password gl4nc3 glance openstack role add --project service --user glance admin Creamos el servicio glance : openstack service create --name glance --description \"OpenStack Image\" image Creamos los endpoints para Glance: openstack endpoint create --region SanSebastian image public http://public-api.hispavistaroot.local:9292 openstack endpoint create --region SanSebastian image internal http://controller:9292 openstack endpoint create --region SanSebastian image admin http://controller:9292 2.4 Configuraci\u00f3n controllers Configuramos glance-api editando el fichero /etc/glance/glance-api.conf : [DEFAULT] bind_host = controller0x bind_port = 9292 registry_host = controller registry_port = 9191 [database] #connection = <None> connection = mysql+pymysql://glanceuser:glancepass@controller/glance enable_v1_api=False enable_v2_api=True [keystone_authtoken] auth_uri = http://public-api.hispavistaroot.local:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = glance password = gl4nc3 [paste_deploy] # ... flavor = keystone [glance_store] # ... stores = file,http default_store = file filesystem_store_datadir = /var/lib/glance/images/ Configuramos glance-registry editando el fichero /etc/glance/glance-registry.conf : [DEFAULT] bind_host = controller0x bind_port = 9191 [database] #connection = <None> connection = mysql+pymysql://glanceuser:glancepass@controller/glance [keystone_authtoken] auth_uri = http://public-api.hispavistaroot.local:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = glance password = gl4nc3 service_token_roles_required = true memcached_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [paste_deploy] # ... flavor = keystone Reiniciamos los servicios: sudo service glance-registry restart sudo service glance-api restart NOTA: Si queremos que las im\u00e1genes de Glance est\u00e9n en ambos controllers, o montamos /var/lib/glance/images/ por nfs o usamos el m\u00e9todo de rsync de fernet /etc/cron.d/rsync_glance : 0 * * * * root /usr/bin/rsync -av /var/lib/glance/images/ controller02:/var/lib/glance/images/ >/dev/null 2>&1 2.5 Comprobaci\u00f3n 2.5.1 Administrador controller01 Descargamos una imagen Cirros: wget http://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img Subimos la imagen a Glance haci\u00e9ndola p\u00fablica: source adminrc openstack image create \"Cirros 0.3.5\" --file cirros-0.3.5-x86_64-disk.img --disk-format qcow2 --container-format bare --public Comprobamos que se ha subido correctamente: openstack image list 2.5.2 Usuario local Descargamos una imagen de Ubuntu: wget https://cloud-images.ubuntu.com/xenial/20180627/xenial-server-cloudimg-amd64-disk1.img Subimos la imagen a Glance haci\u00e9ndola privada: source hvsistemasrc openstack image create --file xenial-server-cloudimg-amd64-disk1.img --disk-format qcow2 --container-format bare --private \"Ubuntu 16.04 (amd64)\" Comprobamos que se ha subido correctamente: openstack image list Para a\u00f1adir los metadefs hay que ejecutar el siguiente comando: su -s /bin/sh -c \"glance-manage db_load_metadefs\" glance HA Es importante que los repositorios de im\u00e1genes est\u00e9n sincronizados, ya sea por NFS, CEPH, rsync o lo que sea, ya que si no no podremos crear instancias.","title":"2. - Glance"},{"location":"openstack/02-glance/#2-glance","text":"","title":"2. Glance"},{"location":"openstack/02-glance/#21-instalacion","text":"controllers NOTA: a d\u00eda 08/02/2018 no hemos podido implementar la v3 de la api de Cinder en Horizon, no la soporta. Instalamos los paquetes para Glance: sudo apt install glance","title":"2.1 Instalaci\u00f3n"},{"location":"openstack/02-glance/#22-base-de-datos","text":"controller01 Creamos la base de datos: sudo mysql -uroot -p CREATE DATABASE glance; GRANT ALL PRIVILEGES ON glance.* TO 'glanceuser'@'localhost' IDENTIFIED BY 'glancepass'; GRANT ALL PRIVILEGES ON glance.* TO 'glanceuser'@'%' IDENTIFIED BY 'glancepass'; Controllers Editamos el fichero /etc/glance/glance-api.conf : [database] #connection = <None> connection = mysql+pymysql://glanceuser:glancepass@controller/glance Controller01 Poblamos la base de datos: sudo su -s /bin/sh -c \"glance-manage db_sync\" glance","title":"2.2 Base de datos"},{"location":"openstack/02-glance/#23-servicio-usuario-y-endpoints","text":"controller01 Creamos el usuario glance y lo a\u00f1adimos al proyecto service con el rol admin : source adminrc openstack user create --domain default --password gl4nc3 glance openstack role add --project service --user glance admin Creamos el servicio glance : openstack service create --name glance --description \"OpenStack Image\" image Creamos los endpoints para Glance: openstack endpoint create --region SanSebastian image public http://public-api.hispavistaroot.local:9292 openstack endpoint create --region SanSebastian image internal http://controller:9292 openstack endpoint create --region SanSebastian image admin http://controller:9292","title":"2.3 Servicio, usuario y endpoints"},{"location":"openstack/02-glance/#24-configuracion","text":"controllers Configuramos glance-api editando el fichero /etc/glance/glance-api.conf : [DEFAULT] bind_host = controller0x bind_port = 9292 registry_host = controller registry_port = 9191 [database] #connection = <None> connection = mysql+pymysql://glanceuser:glancepass@controller/glance enable_v1_api=False enable_v2_api=True [keystone_authtoken] auth_uri = http://public-api.hispavistaroot.local:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = glance password = gl4nc3 [paste_deploy] # ... flavor = keystone [glance_store] # ... stores = file,http default_store = file filesystem_store_datadir = /var/lib/glance/images/ Configuramos glance-registry editando el fichero /etc/glance/glance-registry.conf : [DEFAULT] bind_host = controller0x bind_port = 9191 [database] #connection = <None> connection = mysql+pymysql://glanceuser:glancepass@controller/glance [keystone_authtoken] auth_uri = http://public-api.hispavistaroot.local:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = glance password = gl4nc3 service_token_roles_required = true memcached_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [paste_deploy] # ... flavor = keystone Reiniciamos los servicios: sudo service glance-registry restart sudo service glance-api restart NOTA: Si queremos que las im\u00e1genes de Glance est\u00e9n en ambos controllers, o montamos /var/lib/glance/images/ por nfs o usamos el m\u00e9todo de rsync de fernet /etc/cron.d/rsync_glance : 0 * * * * root /usr/bin/rsync -av /var/lib/glance/images/ controller02:/var/lib/glance/images/ >/dev/null 2>&1","title":"2.4 Configuraci\u00f3n"},{"location":"openstack/02-glance/#25-comprobacion","text":"","title":"2.5 Comprobaci\u00f3n"},{"location":"openstack/02-glance/#251-administrador","text":"controller01 Descargamos una imagen Cirros: wget http://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img Subimos la imagen a Glance haci\u00e9ndola p\u00fablica: source adminrc openstack image create \"Cirros 0.3.5\" --file cirros-0.3.5-x86_64-disk.img --disk-format qcow2 --container-format bare --public Comprobamos que se ha subido correctamente: openstack image list","title":"2.5.1 Administrador"},{"location":"openstack/02-glance/#252-usuario","text":"local Descargamos una imagen de Ubuntu: wget https://cloud-images.ubuntu.com/xenial/20180627/xenial-server-cloudimg-amd64-disk1.img Subimos la imagen a Glance haci\u00e9ndola privada: source hvsistemasrc openstack image create --file xenial-server-cloudimg-amd64-disk1.img --disk-format qcow2 --container-format bare --private \"Ubuntu 16.04 (amd64)\" Comprobamos que se ha subido correctamente: openstack image list Para a\u00f1adir los metadefs hay que ejecutar el siguiente comando: su -s /bin/sh -c \"glance-manage db_load_metadefs\" glance","title":"2.5.2 Usuario"},{"location":"openstack/02-glance/#ha","text":"Es importante que los repositorios de im\u00e1genes est\u00e9n sincronizados, ya sea por NFS, CEPH, rsync o lo que sea, ya que si no no podremos crear instancias.","title":"HA"},{"location":"openstack/03-nova/","text":"3. Nova 3.0 Teor\u00eda Cells Placement API 3.1 Instalaci\u00f3n 3.1.1 Controladores controller01 Instalamos los paquetes para Nova: sudo apt install nova-api nova-conductor nova-consoleauth nova-novncproxy nova-scheduler nova-placement-api 3.1.2 Computaci\u00f3n kvm01 kvm02 Instalamos los paquetes para Nova: sudo apt install nova-compute 3.2 Base de datos controller01 Creamos las bases de datos: sudo mysql -p CREATE DATABASE nova; GRANT ALL PRIVILEGES ON nova.* TO 'novauser'@'localhost' IDENTIFIED BY 'novapass'; GRANT ALL PRIVILEGES ON nova.* TO 'novauser'@'%' IDENTIFIED BY 'novapass'; CREATE DATABASE nova_api; GRANT ALL PRIVILEGES ON nova_api.* TO 'novauser'@'localhost' IDENTIFIED BY 'novapass'; GRANT ALL PRIVILEGES ON nova_api.* TO 'novauser'@'%' IDENTIFIED BY 'novapass'; CREATE DATABASE nova_cell0; GRANT ALL PRIVILEGES ON nova_cell0.* TO 'novauser'@'localhost' IDENTIFIED BY 'novapass'; GRANT ALL PRIVILEGES ON nova_cell0.* TO 'novauser'@'%' IDENTIFIED BY 'novapass'; 3.3 Servicios, usuarios y endpoints controller01 Creamos el usuario nova y lo a\u00f1adimos al proyecto service con el rol admin : source adminrc openstack user create --domain default --password n0v4 nova openstack role add --project service --user nova admin Creamos el servicio nova : openstack service create --name nova --description \"OpenStack Compute\" compute Creamos los endpoints para Nova: openstack endpoint create --region SanSebastian compute public http://public-api.hispavistaroot.local:8774/v2.1 openstack endpoint create --region SanSebastian compute internal http://controller:8774/v2.1 openstack endpoint create --region SanSebastian compute admin http://controller:8774/v2.1 Creamos el usuario placement y lo a\u00f1adimos al proyecto service con el rol admin : source adminrc openstack user create --domain default --password pl4c3m3nt placement openstack role add --project service --user placement admin Creamos el servicio placement : openstack service create --name placement --description \"Placement API\" placement Creamos los endpoints para Placement: openstack endpoint create --region SanSebastian placement public http://public-api.hispavistaroot.local:8778 openstack endpoint create --region SanSebastian placement internal http://controller:8778 openstack endpoint create --region SanSebastian placement admin http://controller:8778 3.4 Configuraci\u00f3n 3.4.1 Controladores controllers Configuramos Nova editando el fichero /etc/nova/nova.conf : [DEFAULT] transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack bind_host = controller0x my_ip = controller0X use_neutron = true firewall_driver = nova.virt.firewall.NoopFirewallDriver osapi_compute_listen = $my_ip [database] connection = mysql+pymysql://novauser:novapass@controller/nova [api_database] connection = mysql+pymysql://novauser:novapass@controller/nova_api [cache] backend = dogpile.cache.memcached enabled = true memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [api] auth_strategy = keystone [keystone_authtoken] auth_uri = http://public-api.hispavistaroot.local:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = nova password = n0v4 backend = dogpile.cache.memcached memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [vnc] enabled = true vncserver_listen = $my_ip vncserver_proxyclient_address = $my_ip novncproxy_host = $my_ip [glance] api_servers = http://controller:9292 [oslo_concurrency] lock_path = /var/lib/nova/tmp [placement] os_region_name = SanSebastian project_domain_name = Default project_name = service auth_type = password user_domain_name = Default auth_url = http://controller:35357/v3 username = placement password = pl4c3m3nt [oslo_messaging_rabbit] rabbit_retry_interval = 1 rabbit_retry_backoff = 2 rabbit_max_retries = 0 rabbit_ha_queues = true 3.4.2 Computaci\u00f3n kvm01 kvm02 Configuramos Nova editando el fichero /etc/nova/nova.conf : [DEFAULT] transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack my_ip = 172.16.100.10x use_neutron = true firewall_driver = nova.virt.firewall.NoopFirewallDriver block_device_allocate_retries = 600 block_device_allocate_retries_interval = 10 [api] auth_strategy = keystone [keystone_authtoken] auth_uri = http://public-api.hispavistaroot.local:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = nova password = n0v4 backend = dogpile.cache.memcached memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [vnc] enabled = true vncserver_listen = 0.0.0.0 vncserver_proxyclient_address = $my_ip novncproxy_base_url = http://public-api.hispavistaroot.local:6080/vnc_auto.html [glance] api_servers = http://controller:9292 [oslo_concurrency] lock_path = /var/lib/nova/tmp [placement] os_region_name = SanSebastian project_domain_name = Default project_name = service auth_type = password user_domain_name = Default auth_url = http://controller:35357/v3 os_interface = admin username = placement password = pl4c3m3nt NOTA: Nos aseguramos que hemos cambiado la x por 1 en caso de kvm01 y 2 en caso de kvm02 Configuramos el tipo de virtualizaci\u00f3n editando el fichero /etc/nova/nova-compute.conf : [libvirt] virt_type = kvm 3.5 Inicializaci\u00f3n 3.5.1 Controladores controller01 Inicializamos las bases de datos: sudo su -s /bin/sh -c \"nova-manage api_db sync\" nova sudo su -s /bin/sh -c \"nova-manage cell_v2 map_cell0\" nova sudo su -s /bin/sh -c \"nova-manage cell_v2 create_cell --name=cell1 --verbose\" nova sudo su -s /bin/sh -c \"nova-manage db sync\" nova Comprobamos que se han registrado correctamente cell0 y cell1 : sudo nova-manage cell_v2 list_cells Reiniciamos los servicios: sudo service nova-api restart sudo service nova-consoleauth restart sudo service nova-scheduler restart sudo service nova-conductor restart sudo service nova-novncproxy restart o service nova-api restart && service nova-consoleauth restart && service nova-scheduler restart && service nova-conductor restart && service nova-novncproxy restart 3.5.2 Computaci\u00f3n kvm01 kvm02 Reiniciamos los servicios: sudo service nova-compute restart Por si acaso reiniciamos los KVM controller01 Damos de alta los nodos de computaci\u00f3n: sudo nova-manage cell_v2 discover_hosts --verbose Found 2 cell mappings. Skipping cell0 since it does not contain hosts. Getting compute nodes from cell 'cell1': 5f4088aa-48c1-4aba-bdfa-b92b4a43e161 Found 2 unmapped computes in cell: 5f4088aa-48c1-4aba-bdfa-b92b4a43e161 Checking host mapping for compute host 'kvm01': 36352569-a283-462d-8c45-8be6a18e9ea9 Creating host mapping for compute host 'kvm01': 36352569-a283-462d-8c45-8be6a18e9ea9 Checking host mapping for compute host 'kvm02': 2593bb79-bfbf-4d6e-bdf4-5ed5559e913d Creating host mapping for compute host 'kvm02': 2593bb79-bfbf-4d6e-bdf4-5ed5559e913d El resultado de nova-status upgrade check : +---------------------------+ | Upgrade Check Results | +---------------------------+ | Check: Cells v2 | | Result: Success | | Details: None | +---------------------------+ | Check: Placement API | | Result: Success | | Details: None | +---------------------------+ | Check: Resource Providers | | Result: Success | | Details: None | +---------------------------+ 3.6 Comprobaci\u00f3n controller01 source adminrc openstack compute service list --service nova-compute openstack compute service list openstack catalog list openstack image list sudo nova-status upgrade check 3.7. Extra kvm01 y kvm02 Si queremos reanudar el estado de las instancias al reiniciar el host, hay que a\u00f1adir lo siguiente a /etc/nova/nova.conf : resume_guests_state_on_host_boot = true En el fichero /etc/nova/nova.conf en hay que apuntar a la IP y no al nombre de host en la siguiente l\u00ednea: novncproxy_base_url = http://public-api.hispavistaroot.local:6080/vnc_auto.html 3.8. Borrar celdas: nova-manage cell_v2 list_cells nova-manage cell_v2 delete_cell --force --cell_uuid XXXXXXXXXXXXXXXXXXXX 3.9. Buscar el host de computacion en un celda mas insistetemente: nova-manage cell_v2 discover_hosts --verbose --by-service","title":"3. - Nova"},{"location":"openstack/03-nova/#3-nova","text":"","title":"3. Nova"},{"location":"openstack/03-nova/#30-teoria","text":"Cells Placement API","title":"3.0 Teor\u00eda"},{"location":"openstack/03-nova/#31-instalacion","text":"","title":"3.1 Instalaci\u00f3n"},{"location":"openstack/03-nova/#311-controladores","text":"controller01 Instalamos los paquetes para Nova: sudo apt install nova-api nova-conductor nova-consoleauth nova-novncproxy nova-scheduler nova-placement-api","title":"3.1.1 Controladores"},{"location":"openstack/03-nova/#312-computacion","text":"kvm01 kvm02 Instalamos los paquetes para Nova: sudo apt install nova-compute","title":"3.1.2 Computaci\u00f3n"},{"location":"openstack/03-nova/#32-base-de-datos","text":"controller01 Creamos las bases de datos: sudo mysql -p CREATE DATABASE nova; GRANT ALL PRIVILEGES ON nova.* TO 'novauser'@'localhost' IDENTIFIED BY 'novapass'; GRANT ALL PRIVILEGES ON nova.* TO 'novauser'@'%' IDENTIFIED BY 'novapass'; CREATE DATABASE nova_api; GRANT ALL PRIVILEGES ON nova_api.* TO 'novauser'@'localhost' IDENTIFIED BY 'novapass'; GRANT ALL PRIVILEGES ON nova_api.* TO 'novauser'@'%' IDENTIFIED BY 'novapass'; CREATE DATABASE nova_cell0; GRANT ALL PRIVILEGES ON nova_cell0.* TO 'novauser'@'localhost' IDENTIFIED BY 'novapass'; GRANT ALL PRIVILEGES ON nova_cell0.* TO 'novauser'@'%' IDENTIFIED BY 'novapass';","title":"3.2 Base de datos"},{"location":"openstack/03-nova/#33-servicios-usuarios-y-endpoints","text":"controller01 Creamos el usuario nova y lo a\u00f1adimos al proyecto service con el rol admin : source adminrc openstack user create --domain default --password n0v4 nova openstack role add --project service --user nova admin Creamos el servicio nova : openstack service create --name nova --description \"OpenStack Compute\" compute Creamos los endpoints para Nova: openstack endpoint create --region SanSebastian compute public http://public-api.hispavistaroot.local:8774/v2.1 openstack endpoint create --region SanSebastian compute internal http://controller:8774/v2.1 openstack endpoint create --region SanSebastian compute admin http://controller:8774/v2.1 Creamos el usuario placement y lo a\u00f1adimos al proyecto service con el rol admin : source adminrc openstack user create --domain default --password pl4c3m3nt placement openstack role add --project service --user placement admin Creamos el servicio placement : openstack service create --name placement --description \"Placement API\" placement Creamos los endpoints para Placement: openstack endpoint create --region SanSebastian placement public http://public-api.hispavistaroot.local:8778 openstack endpoint create --region SanSebastian placement internal http://controller:8778 openstack endpoint create --region SanSebastian placement admin http://controller:8778","title":"3.3 Servicios, usuarios y endpoints"},{"location":"openstack/03-nova/#34-configuracion","text":"","title":"3.4 Configuraci\u00f3n"},{"location":"openstack/03-nova/#341-controladores","text":"controllers Configuramos Nova editando el fichero /etc/nova/nova.conf : [DEFAULT] transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack bind_host = controller0x my_ip = controller0X use_neutron = true firewall_driver = nova.virt.firewall.NoopFirewallDriver osapi_compute_listen = $my_ip [database] connection = mysql+pymysql://novauser:novapass@controller/nova [api_database] connection = mysql+pymysql://novauser:novapass@controller/nova_api [cache] backend = dogpile.cache.memcached enabled = true memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [api] auth_strategy = keystone [keystone_authtoken] auth_uri = http://public-api.hispavistaroot.local:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = nova password = n0v4 backend = dogpile.cache.memcached memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [vnc] enabled = true vncserver_listen = $my_ip vncserver_proxyclient_address = $my_ip novncproxy_host = $my_ip [glance] api_servers = http://controller:9292 [oslo_concurrency] lock_path = /var/lib/nova/tmp [placement] os_region_name = SanSebastian project_domain_name = Default project_name = service auth_type = password user_domain_name = Default auth_url = http://controller:35357/v3 username = placement password = pl4c3m3nt [oslo_messaging_rabbit] rabbit_retry_interval = 1 rabbit_retry_backoff = 2 rabbit_max_retries = 0 rabbit_ha_queues = true","title":"3.4.1 Controladores"},{"location":"openstack/03-nova/#342-computacion","text":"kvm01 kvm02 Configuramos Nova editando el fichero /etc/nova/nova.conf : [DEFAULT] transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack my_ip = 172.16.100.10x use_neutron = true firewall_driver = nova.virt.firewall.NoopFirewallDriver block_device_allocate_retries = 600 block_device_allocate_retries_interval = 10 [api] auth_strategy = keystone [keystone_authtoken] auth_uri = http://public-api.hispavistaroot.local:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = nova password = n0v4 backend = dogpile.cache.memcached memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [vnc] enabled = true vncserver_listen = 0.0.0.0 vncserver_proxyclient_address = $my_ip novncproxy_base_url = http://public-api.hispavistaroot.local:6080/vnc_auto.html [glance] api_servers = http://controller:9292 [oslo_concurrency] lock_path = /var/lib/nova/tmp [placement] os_region_name = SanSebastian project_domain_name = Default project_name = service auth_type = password user_domain_name = Default auth_url = http://controller:35357/v3 os_interface = admin username = placement password = pl4c3m3nt NOTA: Nos aseguramos que hemos cambiado la x por 1 en caso de kvm01 y 2 en caso de kvm02 Configuramos el tipo de virtualizaci\u00f3n editando el fichero /etc/nova/nova-compute.conf : [libvirt] virt_type = kvm","title":"3.4.2 Computaci\u00f3n"},{"location":"openstack/03-nova/#35-inicializacion","text":"","title":"3.5 Inicializaci\u00f3n"},{"location":"openstack/03-nova/#351-controladores","text":"controller01 Inicializamos las bases de datos: sudo su -s /bin/sh -c \"nova-manage api_db sync\" nova sudo su -s /bin/sh -c \"nova-manage cell_v2 map_cell0\" nova sudo su -s /bin/sh -c \"nova-manage cell_v2 create_cell --name=cell1 --verbose\" nova sudo su -s /bin/sh -c \"nova-manage db sync\" nova Comprobamos que se han registrado correctamente cell0 y cell1 : sudo nova-manage cell_v2 list_cells Reiniciamos los servicios: sudo service nova-api restart sudo service nova-consoleauth restart sudo service nova-scheduler restart sudo service nova-conductor restart sudo service nova-novncproxy restart o service nova-api restart && service nova-consoleauth restart && service nova-scheduler restart && service nova-conductor restart && service nova-novncproxy restart","title":"3.5.1 Controladores"},{"location":"openstack/03-nova/#352-computacion","text":"kvm01 kvm02 Reiniciamos los servicios: sudo service nova-compute restart Por si acaso reiniciamos los KVM controller01 Damos de alta los nodos de computaci\u00f3n: sudo nova-manage cell_v2 discover_hosts --verbose Found 2 cell mappings. Skipping cell0 since it does not contain hosts. Getting compute nodes from cell 'cell1': 5f4088aa-48c1-4aba-bdfa-b92b4a43e161 Found 2 unmapped computes in cell: 5f4088aa-48c1-4aba-bdfa-b92b4a43e161 Checking host mapping for compute host 'kvm01': 36352569-a283-462d-8c45-8be6a18e9ea9 Creating host mapping for compute host 'kvm01': 36352569-a283-462d-8c45-8be6a18e9ea9 Checking host mapping for compute host 'kvm02': 2593bb79-bfbf-4d6e-bdf4-5ed5559e913d Creating host mapping for compute host 'kvm02': 2593bb79-bfbf-4d6e-bdf4-5ed5559e913d El resultado de nova-status upgrade check : +---------------------------+ | Upgrade Check Results | +---------------------------+ | Check: Cells v2 | | Result: Success | | Details: None | +---------------------------+ | Check: Placement API | | Result: Success | | Details: None | +---------------------------+ | Check: Resource Providers | | Result: Success | | Details: None | +---------------------------+","title":"3.5.2 Computaci\u00f3n"},{"location":"openstack/03-nova/#36-comprobacion","text":"controller01 source adminrc openstack compute service list --service nova-compute openstack compute service list openstack catalog list openstack image list sudo nova-status upgrade check","title":"3.6 Comprobaci\u00f3n"},{"location":"openstack/03-nova/#37-extra","text":"kvm01 y kvm02 Si queremos reanudar el estado de las instancias al reiniciar el host, hay que a\u00f1adir lo siguiente a /etc/nova/nova.conf : resume_guests_state_on_host_boot = true En el fichero /etc/nova/nova.conf en hay que apuntar a la IP y no al nombre de host en la siguiente l\u00ednea: novncproxy_base_url = http://public-api.hispavistaroot.local:6080/vnc_auto.html","title":"3.7. Extra"},{"location":"openstack/03-nova/#38-borrar-celdas","text":"nova-manage cell_v2 list_cells nova-manage cell_v2 delete_cell --force --cell_uuid XXXXXXXXXXXXXXXXXXXX","title":"3.8. Borrar celdas:"},{"location":"openstack/03-nova/#39-buscar-el-host-de-computacion-en-un-celda-mas-insistetemente","text":"nova-manage cell_v2 discover_hosts --verbose --by-service","title":"3.9. Buscar el host de computacion en un celda mas insistetemente:"},{"location":"openstack/04-neutron/","text":"4. Neutron 4.1 Instalaci\u00f3n 4.1.1 Controladores controllers Instalamos los paquetes para Neutron: sudo apt install neutron-server 4.1.2 Networking gateways Instalamos los paquetes para Neutron: sudo apt install neutron-plugin-ml2 neutron-openvswitch-agent neutron-l3-agent neutron-dhcp-agent neutron-metadata-agent 4.1.3 Computaci\u00f3n kvm01 kvm02 Instalamos los paquetes para Neutron: sudo apt install neutron-openvswitch-agent 4.2 Base de datos controller01 Creamos la base de datos: sudo mysql CREATE DATABASE neutron; GRANT ALL PRIVILEGES ON neutron.* TO 'neutronuser'@'localhost' IDENTIFIED BY 'neutronpass'; GRANT ALL PRIVILEGES ON neutron.* TO 'neutronuser'@'%' IDENTIFIED BY 'neutronpass'; 4.3 Servicio, usuario y endpoints controller01 Creamos el usuario neutron y lo a\u00f1adimos al proyecto service con el rol admin : source adminrc openstack user create --domain default --password n37tr0n neutron openstack role add --project service --user neutron admin Creamos el servicio neutron : openstack service create --name neutron --description \"OpenStack Networking\" network Creamos los endpoints para Neutron: openstack endpoint create --region SanSebastian network public http://public-api.hispavistaroot.local:9696 openstack endpoint create --region SanSebastian network internal http://controller:8778 openstack endpoint create --region SanSebastian network admin http://controller:8778 4.4 Configuraci\u00f3n 4.4.1 Controladores controllers Configuramos Neutron editando el fichero /etc/neutron/neutron.conf : [DEFAULT] bind_host = controller0x bind_port = 9696 transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack auth_strategy = keystone core_plugin = ml2 service_plugins = router allow_overlapping_ips = true notify_nova_on_port_status_changes = true notify_nova_on_port_data_changes = true [database] #connection = sqlite:////var/lib/neutron/neutron.sqlite connection = mysql+pymysql://neutronuser:neutronpass@controller/neutron [keystone_authtoken] auth_uri = http://public-api.hispavistaroot.local:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = n37tr0n backend = dogpile.cache.memcached memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [nova] auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default region_name = SanSebastian project_name = service username = nova password = n0v4 Configuramos el plugin ML2 editando el fichero /etc/neutron/plugins/ml2/ml2_conf.ini : [ml2] type_drivers = flat,vxlan tenant_network_types = vxlan mechanism_drivers = openvswitch,l2population extension_drivers = port_security [ml2_type_flat] flat_networks = provider [ml2_type_vxlan] vni_ranges = 1:1000 [securitygroup] firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver enable_ipset = true Configuramos Nova para que use Neutron editando el fichero /etc/nova/nova.conf : [neutron] url = http://controller:9696 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default region_name = SanSebastian project_name = service username = neutron password = n37tr0n service_metadata_proxy = true metadata_proxy_shared_secret = 1234567890 Reiniciamos los servicios: sudo service neutron-server restart sudo service nova-api restart 4.4.2 Networking gateways En el fichero /etc/sysctl.conf a\u00f1adimos: net.ipv4.ip_forward=1 net.ipv4.conf.all.rp_filter=0 net.ipv4.conf.default.rp_filter=0 Aplicamos los par\u00e1metros: sudo sysctl -p Configuramos Neutron editando el fichero /etc/neutron/neutron.conf : [DEFAULT] transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack auth_strategy = keystone core_plugin = ml2 service_plugins = router allow_overlapping_ips = true [keystone_authtoken] auth_uri = http://public-api.hispavistaroot.local:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = n37tr0n Configuramos el plugin ML2 editando el fichero /etc/neutron/plugins/ml2/ml2_conf.ini : [ml2] type_drivers = flat,vxlan tenant_network_types = vxlan mechanism_drivers = openvswitch,l2population extension_drivers = port_security [ml2_type_flat] flat_networks = provider [ml2_type_vxlan] vni_ranges = 1:1000 [securitygroup] firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver enable_ipset = true Configuramos el agente OpenVSwitch editando el fichero /etc/neutron/plugins/ml2/openvswitch_agent.ini : [ovs] local_ip = 172.16.200.5x bridge_mappings = provider:br-ex [agent] tunnel_types = vxlan [securitygroup] firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver enable_ipset = true Cambiamos la configuraci\u00f3n de la interfaz eno2 (o como se llame) editando el fichero /etc/network/interfaces : #auto eno2 #iface ens4 inet dhcp auto eno2 iface eno2 inet manual up ip address add 0/0 dev $IFACE up ip link set $IFACE up up ip link set $IFACE promisc on down ip link set $IFACE promisc Creamos los bridges y a\u00f1adimos la interfaz eno2 (enp9s0f0, eno1, como se llame) al bridge br-ex : sudo ovs-vsctl add-br br-int #por defecto se suele crear en ubuntu al menos sudo ovs-vsctl add-br br-ex sudo ovs-vsctl add-port br-ex eno2 sudo ovs-vsctl show #para verificar los bridges Configuramos el agente L3 editando el fichero /etc/neutron/l3_agent.ini : [DEFAULT] interface_driver = openvswitch Configuramos el agente DHCP editando el fichero /etc/neutron/dhcp_agent.ini : [DEFAULT] interface_driver = openvswitch ovs_integration_bridge = br-int dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq enable_isolated_metadata = true Configuramos el agente Metadata editando el fichero /etc/neutron/metadata_agent.ini : [DEFAULT] nova_metadata_host = controller nova_metadata_port = 8775 metadata_proxy_shared_secret = 1234567890 controller01 Poblamos la base de datos: sudo su -s /bin/sh -c \"neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head\" neutron NOTA: Esto hay que hacerlo con todos los plugins configurados gateways Reiniciamos los servicios: sudo service neutron-openvswitch-agent restart sudo service neutron-l3-agent restart sudo service neutron-dhcp-agent restart sudo service neutron-metadata-agent restart 4.4.3 Computaci\u00f3n kvm01 kvm02 En el fichero /etc/sysctl.conf a\u00f1adimos: net.ipv4.conf.all.rp_filter=0 net.ipv4.conf.default.rp_filter=0 Aplicamos los par\u00e1metros: sudo sysctl -p Configuramos Neutron editando el fichero /etc/neutron/neutron.conf : [DEFAULT] transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack auth_strategy = keystone [keystone_authtoken] auth_uri = http://public-api.hispavistaroot.local:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = n37tr0n Configuramos el agente OpenVSwitch editando el fichero /etc/neutron/plugins/ml2/openvswitch_agent.ini : [ovs] local_ip = 172.16.200.10x [agent] tunnel_types = vxlan [securitygroup] firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver enable_ipset = true NOTA: Nos aseguramos que hemos cambiado la x por 1 en caso de kvm01 y 2 en caso de kvm02 Configuramos Nova para que use Neutron editando el fichero /etc/nova/nova.conf : [neutron] url = http://controller:9696 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default region_name = SanSebastian project_name = service username = neutron password = n37tr0n Reiniciamos los servicios: sudo service nova-compute restart sudo service neutron-openvswitch-agent restart 4.5 Comprobaciones controler01 Comprobamos los agentes: source adminrc openstack network agent list Comprobamos las redes y routers: source hvsistemasrc openstack network list openstack router list 4.6 Debugging Gateways Para poder hacer las pruebas desde \"dentro\" de una red de OpenStack, podemos hacer lo siguiente: Listar routers y servidores dhcp: ip netns list ping: ip netns exec dhcpID ping IP arp: ip netns exec routerID arp -an entrar en un router: ip netns exec routerID bash KVMs Si las IP's flotantes se quedan \"colgadas\" probablemente haya que reiniciar el servicio de openvswitch en los KVM: service openvswitch-switch restart 4.7 HA Explicaci\u00f3n de VRRP y DVR M\u00e1s info aqu\u00ed Para que los servicios de red est\u00e9n en alta disponibilidad hay que a\u00f1adir lo siguiente en neutron.conf: Controllers /etc/neutron/neutron.conf [DEFAULT] l3_ha = true ##opcionales max_l3_agents_per_router = 3 min_l3_agents_per_router = 2 Reiniciamos neutron-server Gateways En los Gateways en /etc/neutron/l3_agent.ini : [DEFAULT] interface_driver = openvswitch external_network_bridge = En /etc/neutron/plugins/ml2/openvswitch_agent.ini : [ovs] bridge_mappings = provider:br-ex local_ip = 172.16.200.5x [agent] tunnel_types = vxlan l2_population = true [securitygroup] firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver Y reiniciamos neutron-openvswitch-agent y neutron-l3-agent","title":"4. - Neutron"},{"location":"openstack/04-neutron/#4-neutron","text":"","title":"4. Neutron"},{"location":"openstack/04-neutron/#41-instalacion","text":"","title":"4.1 Instalaci\u00f3n"},{"location":"openstack/04-neutron/#411-controladores","text":"controllers Instalamos los paquetes para Neutron: sudo apt install neutron-server","title":"4.1.1 Controladores"},{"location":"openstack/04-neutron/#412-networking","text":"gateways Instalamos los paquetes para Neutron: sudo apt install neutron-plugin-ml2 neutron-openvswitch-agent neutron-l3-agent neutron-dhcp-agent neutron-metadata-agent","title":"4.1.2 Networking"},{"location":"openstack/04-neutron/#413-computacion","text":"kvm01 kvm02 Instalamos los paquetes para Neutron: sudo apt install neutron-openvswitch-agent","title":"4.1.3 Computaci\u00f3n"},{"location":"openstack/04-neutron/#42-base-de-datos","text":"controller01 Creamos la base de datos: sudo mysql CREATE DATABASE neutron; GRANT ALL PRIVILEGES ON neutron.* TO 'neutronuser'@'localhost' IDENTIFIED BY 'neutronpass'; GRANT ALL PRIVILEGES ON neutron.* TO 'neutronuser'@'%' IDENTIFIED BY 'neutronpass';","title":"4.2 Base de datos"},{"location":"openstack/04-neutron/#43-servicio-usuario-y-endpoints","text":"controller01 Creamos el usuario neutron y lo a\u00f1adimos al proyecto service con el rol admin : source adminrc openstack user create --domain default --password n37tr0n neutron openstack role add --project service --user neutron admin Creamos el servicio neutron : openstack service create --name neutron --description \"OpenStack Networking\" network Creamos los endpoints para Neutron: openstack endpoint create --region SanSebastian network public http://public-api.hispavistaroot.local:9696 openstack endpoint create --region SanSebastian network internal http://controller:8778 openstack endpoint create --region SanSebastian network admin http://controller:8778","title":"4.3 Servicio, usuario y endpoints"},{"location":"openstack/04-neutron/#44-configuracion","text":"","title":"4.4 Configuraci\u00f3n"},{"location":"openstack/04-neutron/#441-controladores","text":"controllers Configuramos Neutron editando el fichero /etc/neutron/neutron.conf : [DEFAULT] bind_host = controller0x bind_port = 9696 transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack auth_strategy = keystone core_plugin = ml2 service_plugins = router allow_overlapping_ips = true notify_nova_on_port_status_changes = true notify_nova_on_port_data_changes = true [database] #connection = sqlite:////var/lib/neutron/neutron.sqlite connection = mysql+pymysql://neutronuser:neutronpass@controller/neutron [keystone_authtoken] auth_uri = http://public-api.hispavistaroot.local:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = n37tr0n backend = dogpile.cache.memcached memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [nova] auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default region_name = SanSebastian project_name = service username = nova password = n0v4 Configuramos el plugin ML2 editando el fichero /etc/neutron/plugins/ml2/ml2_conf.ini : [ml2] type_drivers = flat,vxlan tenant_network_types = vxlan mechanism_drivers = openvswitch,l2population extension_drivers = port_security [ml2_type_flat] flat_networks = provider [ml2_type_vxlan] vni_ranges = 1:1000 [securitygroup] firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver enable_ipset = true Configuramos Nova para que use Neutron editando el fichero /etc/nova/nova.conf : [neutron] url = http://controller:9696 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default region_name = SanSebastian project_name = service username = neutron password = n37tr0n service_metadata_proxy = true metadata_proxy_shared_secret = 1234567890 Reiniciamos los servicios: sudo service neutron-server restart sudo service nova-api restart","title":"4.4.1 Controladores"},{"location":"openstack/04-neutron/#442-networking","text":"gateways En el fichero /etc/sysctl.conf a\u00f1adimos: net.ipv4.ip_forward=1 net.ipv4.conf.all.rp_filter=0 net.ipv4.conf.default.rp_filter=0 Aplicamos los par\u00e1metros: sudo sysctl -p Configuramos Neutron editando el fichero /etc/neutron/neutron.conf : [DEFAULT] transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack auth_strategy = keystone core_plugin = ml2 service_plugins = router allow_overlapping_ips = true [keystone_authtoken] auth_uri = http://public-api.hispavistaroot.local:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = n37tr0n Configuramos el plugin ML2 editando el fichero /etc/neutron/plugins/ml2/ml2_conf.ini : [ml2] type_drivers = flat,vxlan tenant_network_types = vxlan mechanism_drivers = openvswitch,l2population extension_drivers = port_security [ml2_type_flat] flat_networks = provider [ml2_type_vxlan] vni_ranges = 1:1000 [securitygroup] firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver enable_ipset = true Configuramos el agente OpenVSwitch editando el fichero /etc/neutron/plugins/ml2/openvswitch_agent.ini : [ovs] local_ip = 172.16.200.5x bridge_mappings = provider:br-ex [agent] tunnel_types = vxlan [securitygroup] firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver enable_ipset = true Cambiamos la configuraci\u00f3n de la interfaz eno2 (o como se llame) editando el fichero /etc/network/interfaces : #auto eno2 #iface ens4 inet dhcp auto eno2 iface eno2 inet manual up ip address add 0/0 dev $IFACE up ip link set $IFACE up up ip link set $IFACE promisc on down ip link set $IFACE promisc Creamos los bridges y a\u00f1adimos la interfaz eno2 (enp9s0f0, eno1, como se llame) al bridge br-ex : sudo ovs-vsctl add-br br-int #por defecto se suele crear en ubuntu al menos sudo ovs-vsctl add-br br-ex sudo ovs-vsctl add-port br-ex eno2 sudo ovs-vsctl show #para verificar los bridges Configuramos el agente L3 editando el fichero /etc/neutron/l3_agent.ini : [DEFAULT] interface_driver = openvswitch Configuramos el agente DHCP editando el fichero /etc/neutron/dhcp_agent.ini : [DEFAULT] interface_driver = openvswitch ovs_integration_bridge = br-int dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq enable_isolated_metadata = true Configuramos el agente Metadata editando el fichero /etc/neutron/metadata_agent.ini : [DEFAULT] nova_metadata_host = controller nova_metadata_port = 8775 metadata_proxy_shared_secret = 1234567890 controller01 Poblamos la base de datos: sudo su -s /bin/sh -c \"neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head\" neutron NOTA: Esto hay que hacerlo con todos los plugins configurados gateways Reiniciamos los servicios: sudo service neutron-openvswitch-agent restart sudo service neutron-l3-agent restart sudo service neutron-dhcp-agent restart sudo service neutron-metadata-agent restart","title":"4.4.2 Networking"},{"location":"openstack/04-neutron/#443-computacion","text":"kvm01 kvm02 En el fichero /etc/sysctl.conf a\u00f1adimos: net.ipv4.conf.all.rp_filter=0 net.ipv4.conf.default.rp_filter=0 Aplicamos los par\u00e1metros: sudo sysctl -p Configuramos Neutron editando el fichero /etc/neutron/neutron.conf : [DEFAULT] transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack auth_strategy = keystone [keystone_authtoken] auth_uri = http://public-api.hispavistaroot.local:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = n37tr0n Configuramos el agente OpenVSwitch editando el fichero /etc/neutron/plugins/ml2/openvswitch_agent.ini : [ovs] local_ip = 172.16.200.10x [agent] tunnel_types = vxlan [securitygroup] firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver enable_ipset = true NOTA: Nos aseguramos que hemos cambiado la x por 1 en caso de kvm01 y 2 en caso de kvm02 Configuramos Nova para que use Neutron editando el fichero /etc/nova/nova.conf : [neutron] url = http://controller:9696 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default region_name = SanSebastian project_name = service username = neutron password = n37tr0n Reiniciamos los servicios: sudo service nova-compute restart sudo service neutron-openvswitch-agent restart","title":"4.4.3 Computaci\u00f3n"},{"location":"openstack/04-neutron/#45-comprobaciones","text":"controler01 Comprobamos los agentes: source adminrc openstack network agent list Comprobamos las redes y routers: source hvsistemasrc openstack network list openstack router list","title":"4.5 Comprobaciones"},{"location":"openstack/04-neutron/#46-debugging","text":"Gateways Para poder hacer las pruebas desde \"dentro\" de una red de OpenStack, podemos hacer lo siguiente: Listar routers y servidores dhcp: ip netns list ping: ip netns exec dhcpID ping IP arp: ip netns exec routerID arp -an entrar en un router: ip netns exec routerID bash KVMs Si las IP's flotantes se quedan \"colgadas\" probablemente haya que reiniciar el servicio de openvswitch en los KVM: service openvswitch-switch restart","title":"4.6 Debugging"},{"location":"openstack/04-neutron/#47-ha","text":"Explicaci\u00f3n de VRRP y DVR M\u00e1s info aqu\u00ed Para que los servicios de red est\u00e9n en alta disponibilidad hay que a\u00f1adir lo siguiente en neutron.conf: Controllers /etc/neutron/neutron.conf [DEFAULT] l3_ha = true ##opcionales max_l3_agents_per_router = 3 min_l3_agents_per_router = 2 Reiniciamos neutron-server Gateways En los Gateways en /etc/neutron/l3_agent.ini : [DEFAULT] interface_driver = openvswitch external_network_bridge = En /etc/neutron/plugins/ml2/openvswitch_agent.ini : [ovs] bridge_mappings = provider:br-ex local_ip = 172.16.200.5x [agent] tunnel_types = vxlan l2_population = true [securitygroup] firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver Y reiniciamos neutron-openvswitch-agent y neutron-l3-agent","title":"4.7 HA"},{"location":"openstack/05-horizon/","text":"5. Horizon 5.1 Instalaci\u00f3n controller01 Instalamos los paquetes para Horizon: sudo apt install openstack-dashboard 5.2 Configuraci\u00f3n controllers Configuramos Horizon editando el fichero /etc/openstack-dashboard/local_settings.py : OPENSTACK_HOST = \"172.16.100.1x\" OPENSTACK_KEYSTONE_URL = \"http://%s:5000/v3\" % OPENSTACK_HOST OPENSTACK_KEYSTONE_DEFAULT_ROLE = \"user\" TIME_ZONE = \"Europe/Madrid\" CACHES = { 'default': { 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': '172.16.100.1x:11211', } } SESSION_ENGINE = 'django.contrib.sessions.backends.cache' OPENSTACK_API_VERSIONS = { \"identity\": 3, \"image\": 2, \"volume\": 2, } DEFAULT_THEME = 'default' Reiniciamos los servicios: sudo service apache2 restart 5.3 Laboratorio 5.3.1 Consolas VNC kvm01 kvm02 Modificamos el acceso a la consola vnc en /etc/nova/nova.conf : novncproxy_base_url = http://public-api.hispavistaroot.local:6080/vnc_auto.html Reiniciamos el servicio: sudo service nova-compute restart 5.4 Comprobaciones internet Accedemos a http://openstack.hispavistaroot.local/horizon y nos logueamos con admin:admin","title":"5. - Horizon"},{"location":"openstack/05-horizon/#5-horizon","text":"","title":"5. Horizon"},{"location":"openstack/05-horizon/#51-instalacion","text":"controller01 Instalamos los paquetes para Horizon: sudo apt install openstack-dashboard","title":"5.1 Instalaci\u00f3n"},{"location":"openstack/05-horizon/#52-configuracion","text":"controllers Configuramos Horizon editando el fichero /etc/openstack-dashboard/local_settings.py : OPENSTACK_HOST = \"172.16.100.1x\" OPENSTACK_KEYSTONE_URL = \"http://%s:5000/v3\" % OPENSTACK_HOST OPENSTACK_KEYSTONE_DEFAULT_ROLE = \"user\" TIME_ZONE = \"Europe/Madrid\" CACHES = { 'default': { 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': '172.16.100.1x:11211', } } SESSION_ENGINE = 'django.contrib.sessions.backends.cache' OPENSTACK_API_VERSIONS = { \"identity\": 3, \"image\": 2, \"volume\": 2, } DEFAULT_THEME = 'default' Reiniciamos los servicios: sudo service apache2 restart","title":"5.2 Configuraci\u00f3n"},{"location":"openstack/05-horizon/#53-laboratorio","text":"","title":"5.3 Laboratorio"},{"location":"openstack/05-horizon/#531-consolas-vnc","text":"kvm01 kvm02 Modificamos el acceso a la consola vnc en /etc/nova/nova.conf : novncproxy_base_url = http://public-api.hispavistaroot.local:6080/vnc_auto.html Reiniciamos el servicio: sudo service nova-compute restart","title":"5.3.1 Consolas VNC"},{"location":"openstack/05-horizon/#54-comprobaciones","text":"internet Accedemos a http://openstack.hispavistaroot.local/horizon y nos logueamos con admin:admin","title":"5.4 Comprobaciones"},{"location":"openstack/06-operaciones/","text":"6. Operaciones 6.0 Revisi\u00f3n 6.0.1 Admin Identity Projects Users Groups Roles Admin Compute Hypervisors Host Aggregates Flavors System Defaults System Information 6.0.2 User Identity Projects Users Project API access Compute Overview Instances Images Key Pairs Network Network Topology Networks Routers Security Groups Floating IPs 6.1 Administraci\u00f3n 6.1.1 Flavors horizon (admin) Creamos los siguientes Flavors : Nombre vCPUs RAM Root disk Ephemeral disk Swap tiny 1 1024 5 0 0 small 2 2048 5 0 0 6.1.2 Networks controller01 Creamos las redes compartidas: source adminrc openstack network create --share --provider-network-type flat --provider-physical-network provider --external provider openstack subnet create --network provider --subnet-range 192.168.111.0/24 --allocation-pool start=192.168.111.80,end=192.168.111.110 --dns-nameserver 8.8.8.8 --gateway 192.168.111.2 provider NOTA: El nombre provider debe coincidir con el nombre que hayamos configurado en Neutron 6.2 Usuario horizon (hvsistemas) Red internal Router router01 Security group default Floating IP Keypair hvsistemas - la copiamos en local Instance ubuntu01 Overview Log Console Action Log Floating IP Ping SSH Interfaz: IP local MTU ( /etc/neutron/plugins/ml2/ml2_conf.ini ) Salida al exterior Flavor: CPU Memoria Disco Snapshots Antes de hacer el snapshot instalamos apache2 y creamos una web sencilla Se bloquea la instancia durante un periodo corto Se crea el snapshot como una imagen (la gestion Glance) Podemos crear instancias a partir de los snapshots Operaciones Pause / Resume Suspend / Resume Shelve (detiene la instancia y hace un snapshot) / Unshelve (levanta la instancia y borra el snapshot) Resize: necesita que el usuario nova pueda acceder a todos los hosts con par de claves Lock: no permite hacer operaciones peligrosas Soft reboot Hard reboot Shutoff / Start Rebuild Delete El resize requiere que los nodos de computaci\u00f3n se comuniquen a trav\u00e9s de ssh sin contrase\u00f1a. kvm01 Creamos un par de claves ssh y las ponemos en el directorio ~nova/.ssh/ : cd ~nova sudo ssh-keygen -b 4096 -t rsa -C \"Key for Nova\" -f id_rsa-nova sudo chmod 400 id_rsa-nova* sudo mkdir .ssh sudo chmod 700 .ssh sudo mv id_rsa-nova* .ssh/ Creamos el fichero ~nova/.ssh/config : Host 172.16.100.10* kvm* IdentityFile ~/.ssh/id_rsa-nova StrictHostKeyChecking no UserKnownHostsFile=/dev/null Creamos el fichero ~nova/.ssh/authorized_keys y a\u00f1adimos la clave p\u00fablica id_rsa-nova.pub Le damos los propietarios de todo al usuario nova : sudo chown -R nova:nova ~nova/.ssh/ Habilitamos la shell para el usuario nova : sudo usermod -s /bin/bash nova kvm02 Creamos el directorio ~nova/.ssh/ : cd ~nova sudo mkdir .ssh Copiamos las claves desde kvm01 a ~nova/.ssh/ Creamos el fichero ~nova/.ssh/config : Host 172.16.100.10* kvm* IdentityFile ~/.ssh/id_rsa-nova StrictHostKeyChecking no UserKnownHostsFile=/dev/null Creamos el fichero ~nova/.ssh/authorized_keys y a\u00f1adimos la clave p\u00fablica id_rsa-nova.pub Le damos los propietarios de todo al usuario nova : sudo chown -R nova:nova ~nova/.ssh/ Habilitamos la shell para el usuario nova : sudo usermod -s /bin/bash nova 6.3 Cloud Init Documentaci\u00f3n Siempre se empieza con #cloud-config Cambiar el username del usuario por defecto: system_info: default_user: name: hvsistemas A\u00f1adir contrase\u00f1a a los usuarios: chpasswd: list: | hvsistemas:hvsistemas expire: False Actualizar paquetes: package_update: true package_upgrade: true Instalar paquetes: packages: - htop - apache2 Escritura de ficheros: write_files: - path: /var/www/html/index.html permissions: '0644' owner: www-data:www-data content: | Mi web Ejecuci\u00f3n de scripts (s\u00f3lo una vez): runcmd: - git clone https://github.com/taigaio/taiga-blog /home/hvsistemas/taiga-blog/","title":"6. - Operaciones"},{"location":"openstack/06-operaciones/#6-operaciones","text":"","title":"6. Operaciones"},{"location":"openstack/06-operaciones/#60-revision","text":"","title":"6.0 Revisi\u00f3n"},{"location":"openstack/06-operaciones/#601-admin","text":"Identity Projects Users Groups Roles Admin Compute Hypervisors Host Aggregates Flavors System Defaults System Information","title":"6.0.1 Admin"},{"location":"openstack/06-operaciones/#602-user","text":"Identity Projects Users Project API access Compute Overview Instances Images Key Pairs Network Network Topology Networks Routers Security Groups Floating IPs","title":"6.0.2 User"},{"location":"openstack/06-operaciones/#61-administracion","text":"","title":"6.1 Administraci\u00f3n"},{"location":"openstack/06-operaciones/#611-flavors","text":"horizon (admin) Creamos los siguientes Flavors : Nombre vCPUs RAM Root disk Ephemeral disk Swap tiny 1 1024 5 0 0 small 2 2048 5 0 0","title":"6.1.1 Flavors"},{"location":"openstack/06-operaciones/#612-networks","text":"controller01 Creamos las redes compartidas: source adminrc openstack network create --share --provider-network-type flat --provider-physical-network provider --external provider openstack subnet create --network provider --subnet-range 192.168.111.0/24 --allocation-pool start=192.168.111.80,end=192.168.111.110 --dns-nameserver 8.8.8.8 --gateway 192.168.111.2 provider NOTA: El nombre provider debe coincidir con el nombre que hayamos configurado en Neutron","title":"6.1.2 Networks"},{"location":"openstack/06-operaciones/#62-usuario","text":"horizon (hvsistemas) Red internal Router router01 Security group default Floating IP Keypair hvsistemas - la copiamos en local Instance ubuntu01 Overview Log Console Action Log Floating IP Ping SSH Interfaz: IP local MTU ( /etc/neutron/plugins/ml2/ml2_conf.ini ) Salida al exterior Flavor: CPU Memoria Disco Snapshots Antes de hacer el snapshot instalamos apache2 y creamos una web sencilla Se bloquea la instancia durante un periodo corto Se crea el snapshot como una imagen (la gestion Glance) Podemos crear instancias a partir de los snapshots Operaciones Pause / Resume Suspend / Resume Shelve (detiene la instancia y hace un snapshot) / Unshelve (levanta la instancia y borra el snapshot) Resize: necesita que el usuario nova pueda acceder a todos los hosts con par de claves Lock: no permite hacer operaciones peligrosas Soft reboot Hard reboot Shutoff / Start Rebuild Delete El resize requiere que los nodos de computaci\u00f3n se comuniquen a trav\u00e9s de ssh sin contrase\u00f1a. kvm01 Creamos un par de claves ssh y las ponemos en el directorio ~nova/.ssh/ : cd ~nova sudo ssh-keygen -b 4096 -t rsa -C \"Key for Nova\" -f id_rsa-nova sudo chmod 400 id_rsa-nova* sudo mkdir .ssh sudo chmod 700 .ssh sudo mv id_rsa-nova* .ssh/ Creamos el fichero ~nova/.ssh/config : Host 172.16.100.10* kvm* IdentityFile ~/.ssh/id_rsa-nova StrictHostKeyChecking no UserKnownHostsFile=/dev/null Creamos el fichero ~nova/.ssh/authorized_keys y a\u00f1adimos la clave p\u00fablica id_rsa-nova.pub Le damos los propietarios de todo al usuario nova : sudo chown -R nova:nova ~nova/.ssh/ Habilitamos la shell para el usuario nova : sudo usermod -s /bin/bash nova kvm02 Creamos el directorio ~nova/.ssh/ : cd ~nova sudo mkdir .ssh Copiamos las claves desde kvm01 a ~nova/.ssh/ Creamos el fichero ~nova/.ssh/config : Host 172.16.100.10* kvm* IdentityFile ~/.ssh/id_rsa-nova StrictHostKeyChecking no UserKnownHostsFile=/dev/null Creamos el fichero ~nova/.ssh/authorized_keys y a\u00f1adimos la clave p\u00fablica id_rsa-nova.pub Le damos los propietarios de todo al usuario nova : sudo chown -R nova:nova ~nova/.ssh/ Habilitamos la shell para el usuario nova : sudo usermod -s /bin/bash nova","title":"6.2 Usuario"},{"location":"openstack/06-operaciones/#63-cloud-init","text":"Documentaci\u00f3n Siempre se empieza con #cloud-config Cambiar el username del usuario por defecto: system_info: default_user: name: hvsistemas A\u00f1adir contrase\u00f1a a los usuarios: chpasswd: list: | hvsistemas:hvsistemas expire: False Actualizar paquetes: package_update: true package_upgrade: true Instalar paquetes: packages: - htop - apache2 Escritura de ficheros: write_files: - path: /var/www/html/index.html permissions: '0644' owner: www-data:www-data content: | Mi web Ejecuci\u00f3n de scripts (s\u00f3lo una vez): runcmd: - git clone https://github.com/taigaio/taiga-blog /home/hvsistemas/taiga-blog/","title":"6.3 Cloud Init"},{"location":"openstack/07-1-cinder+lvm/","text":"7. Cinder 7.0 Info Consistency group 7.1 Instalaci\u00f3n 7.1.1 Controladores controllers Instalamos los paquetes para Cinder: sudo apt install cinder-api cinder-scheduler NOTA: cinder-api se instala como un mod_wsgi de Apache 7.1.2 Almacenamiento storage01 Instalamos los paquetes para Cinder: sudo apt install cinder-volume thin-provisioning-tools 7.2 Base de datos controller01 Creamos la base de datos: sudo mysql -p CREATE DATABASE cinder; GRANT ALL PRIVILEGES ON cinder.* TO 'cinderuser'@'localhost' IDENTIFIED BY 'cinderpass'; GRANT ALL PRIVILEGES ON cinder.* TO 'cinderuser'@'%' IDENTIFIED BY 'cinderpass'; Editamos el fichero /etc/cinder/cinder.conf : [database] connection = mysql+pymysql://cinderuser:cinderpass@controller/cinder Poblamos la base de datos: sudo su -s /bin/sh -c \"cinder-manage db sync\" cinder 7.3 Servicio, usuario y endpoints controller01 Creamos el usuario cinder y lo a\u00f1adimos al proyecto service con el rol admin : source adminrc openstack user create --domain default --password c1nd3r cinder openstack role add --project service --user cinder admin Creamos el servicio cinderv2 : openstack service create --name cinderv2 --description \"OpenStack Block Storage\" volumev2 Creamos los endpoints para Cinder v2: openstack endpoint create --region SanSebastian volumev2 public http://public-api.hispavistaroot.local:8776/v2/%\\(project_id\\)s openstack endpoint create --region SanSebastian volumev2 internal http://controller:8776/v2/%\\(project_id\\)s openstack endpoint create --region SanSebastian volumev2 admin http://controller:8776/v2/%\\(project_id\\)s Creamos el servicio cinderv3 : openstack service create --name cinderv3 --description \"OpenStack Block Storage\" volumev3 Creamos los endpoints para Cinder v3: openstack endpoint create --region SanSebastian volumev3 public http://public-api.hispavistaroot.local:8776/v3/%\\(project_id\\)s openstack endpoint create --region SanSebastian volumev3 internal http://controller:8776/v3/%\\(project_id\\)s openstack endpoint create --region SanSebastian volumev3 admin http://controller:8776/v3/%\\(project_id\\)s 7.4 Configuraci\u00f3n 7.4.1 Controladores controllers Configuramos Cinder editando el fichero /etc/cinder/cinder.conf : [DEFAULT] bind_host = controller0x rootwrap_config = /etc/cinder/rootwrap.conf api_paste_confg = /etc/cinder/api-paste.ini iscsi_helper = tgtadm volume_name_template = volume-%s #volume_group = cinder-volumes verbose = True auth_strategy = keystone state_path = /var/lib/cinder lock_path = /var/lock/cinder volumes_dir = /var/lib/cinder/volumes enabled_backends=lvmFast01,lvmFast02,lvmSlow01 scheduler_driver=cinder.scheduler.filter_scheduler.FilterScheduler transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack auth_host = controllerp auth_strategy = keystone my_ip = 172.16.100.11 [oslo_concurrency] lock_path = /var/lib/cinder/tmp [database] connection = mysql+pymysql://cinderuser:cinderpass@controller/cinder [keystone_authtoken] auth_uri = http://controllerp:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = cinder password = c1nd3r backend = dogpile.cache.memcached memcached_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [cinder] catalog_info = volumev3:cinderv3:adminURL [oslo_messaging_rabbit] rabbit_retry_interval=1 rabbit_retry_backoff=2 rabbit_max_retries=0 rabbit_ha_queues=true Configuramos Nova editando el fichero /etc/nova/nova.conf : [cinder] catalog_info = volumev3:cinderv3:adminURL Reiniciamos los servicios: sudo service nova-api restart sudo service cinder-scheduler restart sudo service apache2 restart 7.4.2 Almacenamiento storage01 Creamos el volumen l\u00f3gico y el grupo de vol\u00famenes cinder-volumes : sudo pvcreate /dev/vdb sudo vgcreate cinder-volumes /dev/vdb Configuramos Cinder editando el fichero /etc/cinder/cinder.conf : [DEFAULT] rootwrap_config = /etc/cinder/rootwrap.conf api_paste_confg = /etc/cinder/api-paste.ini iscsi_helper = tgtadm volume_name_template = volume-%s volume_group = cinder-volumes verbose = False my_ip = 172.16.100.151 glance_catalog_info = image:glance:adminURL transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack auth_strategy = keystone state_path = /var/lib/cinder lock_path = /var/lock/cinder volumes_dir = /var/lib/cinder/volumes enabled_backends = lvmFast01,lvmFast02,lvmSlow01 [oslo_concurrency] lock_path = /var/lib/cinder/tmp [oslo_messaging_notifications] driver = messagingv2 [database] connection = mysql+pymysql://cinderuser:cinderpass@controller/cinder [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = cinder password = c1nd3r [lvmFast01] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinderv1-Fast iscsi_protocol = iscsi iscsi_helper = tgtadm iscsi_ip_address = 172.16.1.151 volume_clear = none volume_backend_name = lvmFast01 [lvmFast02] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinderv2-Fast iscsi_protocol = iscsi iscsi_helper = tgtadm iscsi_ip_address = 172.16.1.151 volume_clear = none volume_backend_name = lvmFast02 [lvmSlow01] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinderv1-Slow iscsi_protocol = iscsi iscsi_helper = tgtadm iscsi_ip_address = 172.16.1.151 volume_clear = none volume_backend_name = lvmSlow01 Reiniciamos los servicios: sudo service tgt restart sudo service cinder-volume restart 7.4.3 Computaci\u00f3n kvm01 kvm02 Configuramos Nova editando el fichero /etc/nova/nova.conf : [cinder] ## hemos puesto el v3 porque no hemos configurado la v2 catalog_info = volumev3:cinderv3:adminURL Reiniciamos el servicio: sudo service nova-compute restart NOTA: Esto es necesario para iniciar instancias desde vol\u00famenes 7.5 Comprobaci\u00f3n 7.5.1 Administrador controller01 A\u00f1adimos la siguiente l\u00ednea al fichero /home/hvsistemas/adminrc : export OS_VOLUME_API_VERSION=3 Comprobamos que est\u00e1n todos los servicios: source adminrc openstack volume service list 7.5.2 Usuario local A\u00f1adimos la siguiente l\u00ednea al fichero /home/hvsistemas/hvsistemasrc : export OS_VOLUME_API_VERSION=3 Creamos un volumen: source hvsistemasrc openstack volume create vol01 --size 1 Listamos los vol\u00famenes: openstack volume list Borramos el volumen: openstack volume delete vol01 7.6 Operaciones Admin (Horizon) En Volume Types creamos los tipos de almacenamiento por cada backend que tengamos, por ejemplo: Disco_Rapido01 --> volume_backend_name --> lvmFast01 Disco_Rapido02 --> volume_backend_name --> lvmFast02 Disco_Lento01 --> volume_backend_name --> lvmSlow01 Para vol\u00famenes \"rebeldes\" si al hacer un lvremove nombredelvolumen nos dice que est\u00e1 en uso: service tgt stop lvremove grupodelvolumen/nombredelvolumen service tgt start horizon (hvsistemas) Creamos un volumen vac\u00edo vol01 de 10GB Hacemos el attach del volumen a una instancia Formateamos Montamos en /mnt Creamos alg\u00fan contenido Hacemos el detach de esta instancia \u00bfPor qu\u00e9 nos pregunta lo que nos pregunta? \u00bfPodemos hacerlel detach sin m\u00e1s? Hacemos el attach a otra instancia Comprobamos que los datos siguen ah\u00ed Hacemos el detach de esta instancia Ampliamos el volumen a 15GB Volvemos a hacer el attach a una instancia Comprobamos el tama\u00f1o del volumen Hacemos un snapshot de vol01 de nombre vol01-snap \u00bfPor qu\u00e9 nos pregunta lo que nos pregunta? Creamos un volumen desde el snapshot vol01-snap de nombre vol02 Hacemos el attach de vol02 a una instancia Comprobamos que los datos siguen ah\u00ed Comprobamos el tama\u00f1o Hacemos el detach de vol02 y lo borramos Hacemos el detach de vol01 y lo borramos \u00bfPor qu\u00e9 da error? Se pueden transferir vol\u00famenes entre proyectos Se puede subir un volumen a una imagen (para arrancar desde \u00e9l) Se puede hacer backup de un volumen (en nuestro caso no) Creaci\u00f3n de instancias creando un volumen Creaci\u00f3n de instancias desde vol\u00famenes Borrar un Servicio de Alamacenamiento de Cinder: lvmFast02 [horizon] Miramos que no tengamos ninguna instancia corriendo en ese backend de disco (migrar a otro volumen) Borramos en Volumen\\Tipos de Volumenes la referencia al backend (Disco_Rapido02) [Storage01] Borramos las entradas que hacen referencia en el fichero /etc/cinder/cinder.conf enabled_backends = lvmFast01,lvmFast02,lvmSlow01 [lvmFast02] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinderv2-Fast iscsi_protocol = iscsi iscsi_helper = tgtadm iscsi_ip_address = 172.16.1.151 volume_clear = none volume_backend_name = lvmFast02 Reiniciamos el servicio volume service cinder-volume restart [Controllers] Modificamos la entrada enabled_backends del fichero /etc/cinder/cinder.conf para que no aparezca el backend eleminado Reiniciamos el servicio cinder scheduler service cinder-scheduler restart Cargamos los sources de admin para openstack cd /home/hvsistemas/ . adminrc Miramos los servicios de volumen que tenemos: openstack volume service list +------------------+---------------------+------+---------+-------+----------------------------+ | Binary | Host | Zone | Status | State | Updated At | +------------------+---------------------+------+---------+-------+----------------------------+ | cinder-scheduler | controller02 | nova | enabled | up | 2018-05-15T10:36:07.000000 | | cinder-scheduler | controller01 | nova | enabled | up | 2018-05-15T10:36:07.000000 | | cinder-volume | storage01@lvmFast01 | nova | enabled | up | 2018-05-15T10:36:08.000000 | | cinder-volume | storage01@lvmFast02 | nova | enabled | up | 2018-05-15T10:36:08.000000 | | cinder-volume | storage01@lvmSlow01 | nova | enabled | up | 2018-05-15T10:36:09.000000 | +------------------+---------------------+------+---------+-------+----------------------------+ Deshabilitamos el servicio storage01@lvmFast02 openstack volume service set storage01@lvmFast02 cinder-volume --disable Borramos el servicio cinder-manage service remove cinder-volume storage01@lvmFast02","title":"7.a. - Cinder con LVM"},{"location":"openstack/07-1-cinder+lvm/#7-cinder","text":"","title":"7. Cinder"},{"location":"openstack/07-1-cinder+lvm/#70-info","text":"Consistency group","title":"7.0 Info"},{"location":"openstack/07-1-cinder+lvm/#71-instalacion","text":"","title":"7.1 Instalaci\u00f3n"},{"location":"openstack/07-1-cinder+lvm/#711-controladores","text":"controllers Instalamos los paquetes para Cinder: sudo apt install cinder-api cinder-scheduler NOTA: cinder-api se instala como un mod_wsgi de Apache","title":"7.1.1 Controladores"},{"location":"openstack/07-1-cinder+lvm/#712-almacenamiento","text":"storage01 Instalamos los paquetes para Cinder: sudo apt install cinder-volume thin-provisioning-tools","title":"7.1.2 Almacenamiento"},{"location":"openstack/07-1-cinder+lvm/#72-base-de-datos","text":"controller01 Creamos la base de datos: sudo mysql -p CREATE DATABASE cinder; GRANT ALL PRIVILEGES ON cinder.* TO 'cinderuser'@'localhost' IDENTIFIED BY 'cinderpass'; GRANT ALL PRIVILEGES ON cinder.* TO 'cinderuser'@'%' IDENTIFIED BY 'cinderpass'; Editamos el fichero /etc/cinder/cinder.conf : [database] connection = mysql+pymysql://cinderuser:cinderpass@controller/cinder Poblamos la base de datos: sudo su -s /bin/sh -c \"cinder-manage db sync\" cinder","title":"7.2 Base de datos"},{"location":"openstack/07-1-cinder+lvm/#73-servicio-usuario-y-endpoints","text":"controller01 Creamos el usuario cinder y lo a\u00f1adimos al proyecto service con el rol admin : source adminrc openstack user create --domain default --password c1nd3r cinder openstack role add --project service --user cinder admin Creamos el servicio cinderv2 : openstack service create --name cinderv2 --description \"OpenStack Block Storage\" volumev2 Creamos los endpoints para Cinder v2: openstack endpoint create --region SanSebastian volumev2 public http://public-api.hispavistaroot.local:8776/v2/%\\(project_id\\)s openstack endpoint create --region SanSebastian volumev2 internal http://controller:8776/v2/%\\(project_id\\)s openstack endpoint create --region SanSebastian volumev2 admin http://controller:8776/v2/%\\(project_id\\)s Creamos el servicio cinderv3 : openstack service create --name cinderv3 --description \"OpenStack Block Storage\" volumev3 Creamos los endpoints para Cinder v3: openstack endpoint create --region SanSebastian volumev3 public http://public-api.hispavistaroot.local:8776/v3/%\\(project_id\\)s openstack endpoint create --region SanSebastian volumev3 internal http://controller:8776/v3/%\\(project_id\\)s openstack endpoint create --region SanSebastian volumev3 admin http://controller:8776/v3/%\\(project_id\\)s","title":"7.3 Servicio, usuario y endpoints"},{"location":"openstack/07-1-cinder+lvm/#74-configuracion","text":"","title":"7.4 Configuraci\u00f3n"},{"location":"openstack/07-1-cinder+lvm/#741-controladores","text":"controllers Configuramos Cinder editando el fichero /etc/cinder/cinder.conf : [DEFAULT] bind_host = controller0x rootwrap_config = /etc/cinder/rootwrap.conf api_paste_confg = /etc/cinder/api-paste.ini iscsi_helper = tgtadm volume_name_template = volume-%s #volume_group = cinder-volumes verbose = True auth_strategy = keystone state_path = /var/lib/cinder lock_path = /var/lock/cinder volumes_dir = /var/lib/cinder/volumes enabled_backends=lvmFast01,lvmFast02,lvmSlow01 scheduler_driver=cinder.scheduler.filter_scheduler.FilterScheduler transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack auth_host = controllerp auth_strategy = keystone my_ip = 172.16.100.11 [oslo_concurrency] lock_path = /var/lib/cinder/tmp [database] connection = mysql+pymysql://cinderuser:cinderpass@controller/cinder [keystone_authtoken] auth_uri = http://controllerp:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = cinder password = c1nd3r backend = dogpile.cache.memcached memcached_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [cinder] catalog_info = volumev3:cinderv3:adminURL [oslo_messaging_rabbit] rabbit_retry_interval=1 rabbit_retry_backoff=2 rabbit_max_retries=0 rabbit_ha_queues=true Configuramos Nova editando el fichero /etc/nova/nova.conf : [cinder] catalog_info = volumev3:cinderv3:adminURL Reiniciamos los servicios: sudo service nova-api restart sudo service cinder-scheduler restart sudo service apache2 restart","title":"7.4.1 Controladores"},{"location":"openstack/07-1-cinder+lvm/#742-almacenamiento","text":"storage01 Creamos el volumen l\u00f3gico y el grupo de vol\u00famenes cinder-volumes : sudo pvcreate /dev/vdb sudo vgcreate cinder-volumes /dev/vdb Configuramos Cinder editando el fichero /etc/cinder/cinder.conf : [DEFAULT] rootwrap_config = /etc/cinder/rootwrap.conf api_paste_confg = /etc/cinder/api-paste.ini iscsi_helper = tgtadm volume_name_template = volume-%s volume_group = cinder-volumes verbose = False my_ip = 172.16.100.151 glance_catalog_info = image:glance:adminURL transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack auth_strategy = keystone state_path = /var/lib/cinder lock_path = /var/lock/cinder volumes_dir = /var/lib/cinder/volumes enabled_backends = lvmFast01,lvmFast02,lvmSlow01 [oslo_concurrency] lock_path = /var/lib/cinder/tmp [oslo_messaging_notifications] driver = messagingv2 [database] connection = mysql+pymysql://cinderuser:cinderpass@controller/cinder [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = cinder password = c1nd3r [lvmFast01] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinderv1-Fast iscsi_protocol = iscsi iscsi_helper = tgtadm iscsi_ip_address = 172.16.1.151 volume_clear = none volume_backend_name = lvmFast01 [lvmFast02] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinderv2-Fast iscsi_protocol = iscsi iscsi_helper = tgtadm iscsi_ip_address = 172.16.1.151 volume_clear = none volume_backend_name = lvmFast02 [lvmSlow01] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinderv1-Slow iscsi_protocol = iscsi iscsi_helper = tgtadm iscsi_ip_address = 172.16.1.151 volume_clear = none volume_backend_name = lvmSlow01 Reiniciamos los servicios: sudo service tgt restart sudo service cinder-volume restart","title":"7.4.2 Almacenamiento"},{"location":"openstack/07-1-cinder+lvm/#743-computacion","text":"kvm01 kvm02 Configuramos Nova editando el fichero /etc/nova/nova.conf : [cinder] ## hemos puesto el v3 porque no hemos configurado la v2 catalog_info = volumev3:cinderv3:adminURL Reiniciamos el servicio: sudo service nova-compute restart NOTA: Esto es necesario para iniciar instancias desde vol\u00famenes","title":"7.4.3 Computaci\u00f3n"},{"location":"openstack/07-1-cinder+lvm/#75-comprobacion","text":"","title":"7.5 Comprobaci\u00f3n"},{"location":"openstack/07-1-cinder+lvm/#751-administrador","text":"controller01 A\u00f1adimos la siguiente l\u00ednea al fichero /home/hvsistemas/adminrc : export OS_VOLUME_API_VERSION=3 Comprobamos que est\u00e1n todos los servicios: source adminrc openstack volume service list","title":"7.5.1 Administrador"},{"location":"openstack/07-1-cinder+lvm/#752-usuario","text":"local A\u00f1adimos la siguiente l\u00ednea al fichero /home/hvsistemas/hvsistemasrc : export OS_VOLUME_API_VERSION=3 Creamos un volumen: source hvsistemasrc openstack volume create vol01 --size 1 Listamos los vol\u00famenes: openstack volume list Borramos el volumen: openstack volume delete vol01","title":"7.5.2 Usuario"},{"location":"openstack/07-1-cinder+lvm/#76-operaciones","text":"Admin (Horizon) En Volume Types creamos los tipos de almacenamiento por cada backend que tengamos, por ejemplo: Disco_Rapido01 --> volume_backend_name --> lvmFast01 Disco_Rapido02 --> volume_backend_name --> lvmFast02 Disco_Lento01 --> volume_backend_name --> lvmSlow01 Para vol\u00famenes \"rebeldes\" si al hacer un lvremove nombredelvolumen nos dice que est\u00e1 en uso: service tgt stop lvremove grupodelvolumen/nombredelvolumen service tgt start horizon (hvsistemas) Creamos un volumen vac\u00edo vol01 de 10GB Hacemos el attach del volumen a una instancia Formateamos Montamos en /mnt Creamos alg\u00fan contenido Hacemos el detach de esta instancia \u00bfPor qu\u00e9 nos pregunta lo que nos pregunta? \u00bfPodemos hacerlel detach sin m\u00e1s? Hacemos el attach a otra instancia Comprobamos que los datos siguen ah\u00ed Hacemos el detach de esta instancia Ampliamos el volumen a 15GB Volvemos a hacer el attach a una instancia Comprobamos el tama\u00f1o del volumen Hacemos un snapshot de vol01 de nombre vol01-snap \u00bfPor qu\u00e9 nos pregunta lo que nos pregunta? Creamos un volumen desde el snapshot vol01-snap de nombre vol02 Hacemos el attach de vol02 a una instancia Comprobamos que los datos siguen ah\u00ed Comprobamos el tama\u00f1o Hacemos el detach de vol02 y lo borramos Hacemos el detach de vol01 y lo borramos \u00bfPor qu\u00e9 da error? Se pueden transferir vol\u00famenes entre proyectos Se puede subir un volumen a una imagen (para arrancar desde \u00e9l) Se puede hacer backup de un volumen (en nuestro caso no) Creaci\u00f3n de instancias creando un volumen Creaci\u00f3n de instancias desde vol\u00famenes Borrar un Servicio de Alamacenamiento de Cinder: lvmFast02 [horizon] Miramos que no tengamos ninguna instancia corriendo en ese backend de disco (migrar a otro volumen) Borramos en Volumen\\Tipos de Volumenes la referencia al backend (Disco_Rapido02) [Storage01] Borramos las entradas que hacen referencia en el fichero /etc/cinder/cinder.conf enabled_backends = lvmFast01,lvmFast02,lvmSlow01 [lvmFast02] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinderv2-Fast iscsi_protocol = iscsi iscsi_helper = tgtadm iscsi_ip_address = 172.16.1.151 volume_clear = none volume_backend_name = lvmFast02 Reiniciamos el servicio volume service cinder-volume restart [Controllers] Modificamos la entrada enabled_backends del fichero /etc/cinder/cinder.conf para que no aparezca el backend eleminado Reiniciamos el servicio cinder scheduler service cinder-scheduler restart Cargamos los sources de admin para openstack cd /home/hvsistemas/ . adminrc Miramos los servicios de volumen que tenemos: openstack volume service list +------------------+---------------------+------+---------+-------+----------------------------+ | Binary | Host | Zone | Status | State | Updated At | +------------------+---------------------+------+---------+-------+----------------------------+ | cinder-scheduler | controller02 | nova | enabled | up | 2018-05-15T10:36:07.000000 | | cinder-scheduler | controller01 | nova | enabled | up | 2018-05-15T10:36:07.000000 | | cinder-volume | storage01@lvmFast01 | nova | enabled | up | 2018-05-15T10:36:08.000000 | | cinder-volume | storage01@lvmFast02 | nova | enabled | up | 2018-05-15T10:36:08.000000 | | cinder-volume | storage01@lvmSlow01 | nova | enabled | up | 2018-05-15T10:36:09.000000 | +------------------+---------------------+------+---------+-------+----------------------------+ Deshabilitamos el servicio storage01@lvmFast02 openstack volume service set storage01@lvmFast02 cinder-volume --disable Borramos el servicio cinder-manage service remove cinder-volume storage01@lvmFast02","title":"7.6 Operaciones"},{"location":"openstack/07-2-cinder+ceph/","text":"7 Cinder Partimos de una instalacion de cinder con lvm 07-1-cinder+lvm.md .... 7.4 Configuraci\u00f3n 7.4.1 Controladores controllers Configuramos Cinder editando el fichero /etc/cinder/cinder.conf : [DEFAULT] bind_host = controller0x rootwrap_config = /etc/cinder/rootwrap.conf api_paste_confg = /etc/cinder/api-paste.ini iscsi_helper = tgtadm volume_name_template = volume-%s #volume_group = cinder-volumes verbose = True auth_strategy = keystone state_path = /var/lib/cinder lock_path = /var/lock/cinder volumes_dir = /var/lib/cinder/volumes enabled_backends = ceph scheduler_driver=cinder.scheduler.filter_scheduler.FilterScheduler transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack auth_host = controllerp auth_strategy = keystone my_ip = 172.16.100.11 [oslo_concurrency] lock_path = /var/lib/cinder/tmp [database] connection = mysql+pymysql://cinderuser:cinderpass@controller/cinder [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = cinder password = c1nd3r backend = dogpile.cache.memcached memcached_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [cinder] catalog_info = volumev3:cinderv3:adminURL [oslo_messaging_rabbit] rabbit_retry_interval=1 rabbit_retry_backoff=2 rabbit_max_retries=0 rabbit_ha_queues=true Configuramos Nova editando el fichero /etc/nova/nova.conf : [cinder] catalog_info = volumev3:cinderv3:adminURL Reiniciamos los servicios: sudo service nova-api restart sudo service cinder-scheduler restart sudo service apache2 restart 7.4.2 Almacenamiento storage01 Configuramos Cinder editando el fichero /etc/cinder/cinder.conf : [DEFAULT] rootwrap_config = /etc/cinder/rootwrap.conf api_paste_confg = /etc/cinder/api-paste.ini iscsi_helper = tgtadm volume_name_template = volume-%s #verbose = True my_ip = 172.16.100.151 glance_catalog_info = image:glance:adminURL transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack auth_strategy = keystone state_path = /var/lib/cinder lock_path = /var/lock/cinder volumes_dir = /var/lib/cinder/volumes enabled_backends = ceph #glance_api_version = 2 [ceph] volume_backend_name=ceph rbd_pool=volumes rbd_user=volumes rbd_secret_uuid=AQDDiP5aFh3uLxAA0We9NNu0Z7ZJZ+VUlcB+MA== #rbd_secret_uuid=AQDBi/1a3YZyLBAA5Jn63AbwJjXjDov0TvJBMg== volume_driver=cinder.volume.drivers.rbd.RBDDriver rbd_ceph_conf=/etc/ceph/ceph.conf backup_driver = cinder.backup.drivers.ceph [oslo_concurrency] lock_path = /var/lib/cinder/tmp [oslo_messaging_notifications] driver = messagingv2 [database] connection = mysql+pymysql://cinderuser:cinderpass@controller/cinder [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = cinder password = c1nd3r Reiniciamos los servicios: sudo service tgt restart sudo service cinder-volume restart 7.4.3 Computaci\u00f3n kvm01 kvm02 Configuramos Nova editando el fichero /etc/nova/nova.conf : [cinder] ## hemos puesto el v3 porque no hemos configurado la v2 catalog_info = volumev3:cinderv3:adminURL Reiniciamos el servicio: sudo service nova-compute restart NOTA: Esto es necesario para iniciar instancias desde vol\u00famenes 7.5 Comprobaci\u00f3n 7.5.1 Administrador controller01 A\u00f1adimos la siguiente l\u00ednea al fichero /home/hvsistemas/adminrc : export OS_VOLUME_API_VERSION=3 Comprobamos que est\u00e1n todos los servicios: source adminrc openstack volume service list 7.5.2 Usuario local A\u00f1adimos la siguiente l\u00ednea al fichero /home/hvsistemas/hvsistemasrc : export OS_VOLUME_API_VERSION=3 Creamos un volumen: source hvsistemasrc openstack volume create vol01 --size 1 Listamos los vol\u00famenes: openstack volume list Borramos el volumen: openstack volume delete vol01 7.6 Operaciones Admin (Horizon) En Volume Types creamos los tipos de almacenamiento por cada backend que tengamos, por ejemplo: Disco_Rapido01 --> volume_backend_name --> lvmFast01 Disco_Rapido02 --> volume_backend_name --> lvmFast02 Disco_Lento01 --> volume_backend_name --> lvmSlow01 Para vol\u00famenes \"rebeldes\" si al hacer un lvremove nombredelvolumen nos dice que est\u00e1 en uso: service tgt stop lvremove grupodelvolumen/nombredelvolumen service tgt start En el anterior metodo al parar el servicio tgt paramos la exportacion de volumenes de Cinder. Con el siguiente metodo no paramos el servicio tgt Sacamos el ID de los volumenes que dan problemas, en un controler: source adminrc openstack volume list --project ionar +--------------------------------------+--------------------+----------------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+--------------------+----------------+------+-------------+ | b6887e4f-ba21-4d4e-98b2-6dbfb0404259 | ceph11-node1-disk4 | error_deleting | 100 | | | 66609490-98be-42f4-b374-bf3d3f1483dc | ceph11-node1-disk2 | error_deleting | 50 | | | ce290bf8-e6e8-4471-8b5c-d18ac10cdd99 | ceph10-node0-disk2 | error_deleting | 50 | | +--------------------------------------+--------------------+----------------+------+-------------+ Nos vamos a la maquina de Cinder y ejecutamos los siguientes comandos dmsetup info -c | grep b6887e4f cinderv1--Slow-volume--b6887e4f--ba21--4d4e--98b2--6dbfb0404259 252 30 L--w 1 1 0 LVM-n3x3zJi7RjmVQncLa1Kz11VeRZwlFGVRs5vvGXmlk3bHBZwPK9g6pelGjSdj88WU ls -la /sys/dev/block/252\\:30/holders/ drwxr-xr-x 2 root root 0 jul 19 09:57 . drwxr-xr-x 9 root root 0 may 3 15:20 .. lrwxrwxrwx 1 root root 0 jul 19 09:46 dm-38 -> ../../dm-38 dmsetup remove /dev/dm-38 Nos volemos a uno de los controlers openstack volume delete b6887e4f-ba21-4d4e-98b2-6dbfb0404259 --force Si listamos otra vez los volumenes tendremos que ver el volumen con error ya no esta: openstack volume list --project ionar +--------------------------------------+--------------------+----------------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+--------------------+----------------+------+-------------+ | 66609490-98be-42f4-b374-bf3d3f1483dc | ceph11-node1-disk2 | error_deleting | 50 | | | ce290bf8-e6e8-4471-8b5c-d18ac10cdd99 | ceph10-node0-disk2 | error_deleting | 50 | | +--------------------------------------+--------------------+----------------+------+-------------+ horizon (hvsistemas) Creamos un volumen vac\u00edo vol01 de 10GB Hacemos el attach del volumen a una instancia Formateamos Montamos en /mnt Creamos alg\u00fan contenido Hacemos el detach de esta instancia \u00bfPor qu\u00e9 nos pregunta lo que nos pregunta? \u00bfPodemos hacerlel detach sin m\u00e1s? Hacemos el attach a otra instancia Comprobamos que los datos siguen ah\u00ed Hacemos el detach de esta instancia Ampliamos el volumen a 15GB Volvemos a hacer el attach a una instancia Comprobamos el tama\u00f1o del volumen Hacemos un snapshot de vol01 de nombre vol01-snap \u00bfPor qu\u00e9 nos pregunta lo que nos pregunta? Creamos un volumen desde el snapshot vol01-snap de nombre vol02 Hacemos el attach de vol02 a una instancia Comprobamos que los datos siguen ah\u00ed Comprobamos el tama\u00f1o Hacemos el detach de vol02 y lo borramos Hacemos el detach de vol01 y lo borramos \u00bfPor qu\u00e9 da error? Se pueden transferir vol\u00famenes entre proyectos Se puede subir un volumen a una imagen (para arrancar desde \u00e9l) Se puede hacer backup de un volumen (en nuestro caso no) Creaci\u00f3n de instancias creando un volumen Creaci\u00f3n de instancias desde vol\u00famenes Borrar un Servicio de Alamacenamiento de Cinder: lvmFast02 [horizon] Miramos que no tengamos ninguna instancia corriendo en ese backend de disco (migrar a otro volumen) Borramos en Volumen\\Tipos de Volumenes la referencia al backend (Disco_Rapido02) [Storage01] Borramos las entradas que hacen referencia en el fichero /etc/cinder/cinder.conf enabled_backends = lvmFast01,lvmFast02,lvmSlow01 [lvmFast02] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinderv2-Fast iscsi_protocol = iscsi iscsi_helper = tgtadm iscsi_ip_address = 172.16.1.151 volume_clear = none volume_backend_name = lvmFast02 Reiniciamos el servicio volume service cinder-volume restart [Controllers] Modificamos la entrada enabled_backends del fichero /etc/cinder/cinder.conf para que no aparezca el backend eleminado Reiniciamos el servicio cinder scheduler service cinder-scheduler restart Cargamos los sources de admin para openstack cd /home/hvsistemas/ . adminrc Miramos los servicios de volumen que tenemos: openstack volume service list +------------------+---------------------+------+---------+-------+----------------------------+ | Binary | Host | Zone | Status | State | Updated At | +------------------+---------------------+------+---------+-------+----------------------------+ | cinder-scheduler | controller02 | nova | enabled | up | 2018-05-15T10:36:07.000000 | | cinder-scheduler | controller01 | nova | enabled | up | 2018-05-15T10:36:07.000000 | | cinder-volume | storage01@lvmFast01 | nova | enabled | up | 2018-05-15T10:36:08.000000 | | cinder-volume | storage01@lvmFast02 | nova | enabled | up | 2018-05-15T10:36:08.000000 | | cinder-volume | storage01@lvmSlow01 | nova | enabled | up | 2018-05-15T10:36:09.000000 | +------------------+---------------------+------+---------+-------+----------------------------+ Deshabilitamos el servicio storage01@lvmFast02 openstack volume service set storage01@lvmFast02 cinder-volume --disable Borramos el servicio cinder-manage service remove cinder-volume storage01@lvmFast02","title":"7.b. - Cinder con Ceph"},{"location":"openstack/07-2-cinder+ceph/#7-cinder","text":"Partimos de una instalacion de cinder con lvm 07-1-cinder+lvm.md ....","title":"7 Cinder"},{"location":"openstack/07-2-cinder+ceph/#74-configuracion","text":"","title":"7.4 Configuraci\u00f3n"},{"location":"openstack/07-2-cinder+ceph/#741-controladores","text":"controllers Configuramos Cinder editando el fichero /etc/cinder/cinder.conf : [DEFAULT] bind_host = controller0x rootwrap_config = /etc/cinder/rootwrap.conf api_paste_confg = /etc/cinder/api-paste.ini iscsi_helper = tgtadm volume_name_template = volume-%s #volume_group = cinder-volumes verbose = True auth_strategy = keystone state_path = /var/lib/cinder lock_path = /var/lock/cinder volumes_dir = /var/lib/cinder/volumes enabled_backends = ceph scheduler_driver=cinder.scheduler.filter_scheduler.FilterScheduler transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack auth_host = controllerp auth_strategy = keystone my_ip = 172.16.100.11 [oslo_concurrency] lock_path = /var/lib/cinder/tmp [database] connection = mysql+pymysql://cinderuser:cinderpass@controller/cinder [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = cinder password = c1nd3r backend = dogpile.cache.memcached memcached_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [cinder] catalog_info = volumev3:cinderv3:adminURL [oslo_messaging_rabbit] rabbit_retry_interval=1 rabbit_retry_backoff=2 rabbit_max_retries=0 rabbit_ha_queues=true Configuramos Nova editando el fichero /etc/nova/nova.conf : [cinder] catalog_info = volumev3:cinderv3:adminURL Reiniciamos los servicios: sudo service nova-api restart sudo service cinder-scheduler restart sudo service apache2 restart","title":"7.4.1 Controladores"},{"location":"openstack/07-2-cinder+ceph/#742-almacenamiento","text":"storage01 Configuramos Cinder editando el fichero /etc/cinder/cinder.conf : [DEFAULT] rootwrap_config = /etc/cinder/rootwrap.conf api_paste_confg = /etc/cinder/api-paste.ini iscsi_helper = tgtadm volume_name_template = volume-%s #verbose = True my_ip = 172.16.100.151 glance_catalog_info = image:glance:adminURL transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack auth_strategy = keystone state_path = /var/lib/cinder lock_path = /var/lock/cinder volumes_dir = /var/lib/cinder/volumes enabled_backends = ceph #glance_api_version = 2 [ceph] volume_backend_name=ceph rbd_pool=volumes rbd_user=volumes rbd_secret_uuid=AQDDiP5aFh3uLxAA0We9NNu0Z7ZJZ+VUlcB+MA== #rbd_secret_uuid=AQDBi/1a3YZyLBAA5Jn63AbwJjXjDov0TvJBMg== volume_driver=cinder.volume.drivers.rbd.RBDDriver rbd_ceph_conf=/etc/ceph/ceph.conf backup_driver = cinder.backup.drivers.ceph [oslo_concurrency] lock_path = /var/lib/cinder/tmp [oslo_messaging_notifications] driver = messagingv2 [database] connection = mysql+pymysql://cinderuser:cinderpass@controller/cinder [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = cinder password = c1nd3r Reiniciamos los servicios: sudo service tgt restart sudo service cinder-volume restart","title":"7.4.2 Almacenamiento"},{"location":"openstack/07-2-cinder+ceph/#743-computacion","text":"kvm01 kvm02 Configuramos Nova editando el fichero /etc/nova/nova.conf : [cinder] ## hemos puesto el v3 porque no hemos configurado la v2 catalog_info = volumev3:cinderv3:adminURL Reiniciamos el servicio: sudo service nova-compute restart NOTA: Esto es necesario para iniciar instancias desde vol\u00famenes","title":"7.4.3 Computaci\u00f3n"},{"location":"openstack/07-2-cinder+ceph/#75-comprobacion","text":"","title":"7.5 Comprobaci\u00f3n"},{"location":"openstack/07-2-cinder+ceph/#751-administrador","text":"controller01 A\u00f1adimos la siguiente l\u00ednea al fichero /home/hvsistemas/adminrc : export OS_VOLUME_API_VERSION=3 Comprobamos que est\u00e1n todos los servicios: source adminrc openstack volume service list","title":"7.5.1 Administrador"},{"location":"openstack/07-2-cinder+ceph/#752-usuario","text":"local A\u00f1adimos la siguiente l\u00ednea al fichero /home/hvsistemas/hvsistemasrc : export OS_VOLUME_API_VERSION=3 Creamos un volumen: source hvsistemasrc openstack volume create vol01 --size 1 Listamos los vol\u00famenes: openstack volume list Borramos el volumen: openstack volume delete vol01","title":"7.5.2 Usuario"},{"location":"openstack/07-2-cinder+ceph/#76-operaciones","text":"Admin (Horizon) En Volume Types creamos los tipos de almacenamiento por cada backend que tengamos, por ejemplo: Disco_Rapido01 --> volume_backend_name --> lvmFast01 Disco_Rapido02 --> volume_backend_name --> lvmFast02 Disco_Lento01 --> volume_backend_name --> lvmSlow01 Para vol\u00famenes \"rebeldes\" si al hacer un lvremove nombredelvolumen nos dice que est\u00e1 en uso: service tgt stop lvremove grupodelvolumen/nombredelvolumen service tgt start En el anterior metodo al parar el servicio tgt paramos la exportacion de volumenes de Cinder. Con el siguiente metodo no paramos el servicio tgt Sacamos el ID de los volumenes que dan problemas, en un controler: source adminrc openstack volume list --project ionar +--------------------------------------+--------------------+----------------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+--------------------+----------------+------+-------------+ | b6887e4f-ba21-4d4e-98b2-6dbfb0404259 | ceph11-node1-disk4 | error_deleting | 100 | | | 66609490-98be-42f4-b374-bf3d3f1483dc | ceph11-node1-disk2 | error_deleting | 50 | | | ce290bf8-e6e8-4471-8b5c-d18ac10cdd99 | ceph10-node0-disk2 | error_deleting | 50 | | +--------------------------------------+--------------------+----------------+------+-------------+ Nos vamos a la maquina de Cinder y ejecutamos los siguientes comandos dmsetup info -c | grep b6887e4f cinderv1--Slow-volume--b6887e4f--ba21--4d4e--98b2--6dbfb0404259 252 30 L--w 1 1 0 LVM-n3x3zJi7RjmVQncLa1Kz11VeRZwlFGVRs5vvGXmlk3bHBZwPK9g6pelGjSdj88WU ls -la /sys/dev/block/252\\:30/holders/ drwxr-xr-x 2 root root 0 jul 19 09:57 . drwxr-xr-x 9 root root 0 may 3 15:20 .. lrwxrwxrwx 1 root root 0 jul 19 09:46 dm-38 -> ../../dm-38 dmsetup remove /dev/dm-38 Nos volemos a uno de los controlers openstack volume delete b6887e4f-ba21-4d4e-98b2-6dbfb0404259 --force Si listamos otra vez los volumenes tendremos que ver el volumen con error ya no esta: openstack volume list --project ionar +--------------------------------------+--------------------+----------------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+--------------------+----------------+------+-------------+ | 66609490-98be-42f4-b374-bf3d3f1483dc | ceph11-node1-disk2 | error_deleting | 50 | | | ce290bf8-e6e8-4471-8b5c-d18ac10cdd99 | ceph10-node0-disk2 | error_deleting | 50 | | +--------------------------------------+--------------------+----------------+------+-------------+ horizon (hvsistemas) Creamos un volumen vac\u00edo vol01 de 10GB Hacemos el attach del volumen a una instancia Formateamos Montamos en /mnt Creamos alg\u00fan contenido Hacemos el detach de esta instancia \u00bfPor qu\u00e9 nos pregunta lo que nos pregunta? \u00bfPodemos hacerlel detach sin m\u00e1s? Hacemos el attach a otra instancia Comprobamos que los datos siguen ah\u00ed Hacemos el detach de esta instancia Ampliamos el volumen a 15GB Volvemos a hacer el attach a una instancia Comprobamos el tama\u00f1o del volumen Hacemos un snapshot de vol01 de nombre vol01-snap \u00bfPor qu\u00e9 nos pregunta lo que nos pregunta? Creamos un volumen desde el snapshot vol01-snap de nombre vol02 Hacemos el attach de vol02 a una instancia Comprobamos que los datos siguen ah\u00ed Comprobamos el tama\u00f1o Hacemos el detach de vol02 y lo borramos Hacemos el detach de vol01 y lo borramos \u00bfPor qu\u00e9 da error? Se pueden transferir vol\u00famenes entre proyectos Se puede subir un volumen a una imagen (para arrancar desde \u00e9l) Se puede hacer backup de un volumen (en nuestro caso no) Creaci\u00f3n de instancias creando un volumen Creaci\u00f3n de instancias desde vol\u00famenes Borrar un Servicio de Alamacenamiento de Cinder: lvmFast02 [horizon] Miramos que no tengamos ninguna instancia corriendo en ese backend de disco (migrar a otro volumen) Borramos en Volumen\\Tipos de Volumenes la referencia al backend (Disco_Rapido02) [Storage01] Borramos las entradas que hacen referencia en el fichero /etc/cinder/cinder.conf enabled_backends = lvmFast01,lvmFast02,lvmSlow01 [lvmFast02] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinderv2-Fast iscsi_protocol = iscsi iscsi_helper = tgtadm iscsi_ip_address = 172.16.1.151 volume_clear = none volume_backend_name = lvmFast02 Reiniciamos el servicio volume service cinder-volume restart [Controllers] Modificamos la entrada enabled_backends del fichero /etc/cinder/cinder.conf para que no aparezca el backend eleminado Reiniciamos el servicio cinder scheduler service cinder-scheduler restart Cargamos los sources de admin para openstack cd /home/hvsistemas/ . adminrc Miramos los servicios de volumen que tenemos: openstack volume service list +------------------+---------------------+------+---------+-------+----------------------------+ | Binary | Host | Zone | Status | State | Updated At | +------------------+---------------------+------+---------+-------+----------------------------+ | cinder-scheduler | controller02 | nova | enabled | up | 2018-05-15T10:36:07.000000 | | cinder-scheduler | controller01 | nova | enabled | up | 2018-05-15T10:36:07.000000 | | cinder-volume | storage01@lvmFast01 | nova | enabled | up | 2018-05-15T10:36:08.000000 | | cinder-volume | storage01@lvmFast02 | nova | enabled | up | 2018-05-15T10:36:08.000000 | | cinder-volume | storage01@lvmSlow01 | nova | enabled | up | 2018-05-15T10:36:09.000000 | +------------------+---------------------+------+---------+-------+----------------------------+ Deshabilitamos el servicio storage01@lvmFast02 openstack volume service set storage01@lvmFast02 cinder-volume --disable Borramos el servicio cinder-manage service remove cinder-volume storage01@lvmFast02","title":"7.6 Operaciones"},{"location":"openstack/08-heat/","text":"8. Heat 8.1 Instalaci\u00f3n controllers Instalamos los paquetes para Heat: sudo apt install heat-api heat-api-cfn heat-engine 8.2 Base de datos controller01 Creamos la base de datos para Heat: sudo mysql -p CREATE DATABASE heat; GRANT ALL PRIVILEGES ON heat.* TO 'heatuser'@'localhost' IDENTIFIED BY 'heatpass'; GRANT ALL PRIVILEGES ON heat.* TO 'heatuser'@'%' IDENTIFIED BY 'heatpass'; Controllers Editamos el fichero /etc/heat/heat.conf : [database] connection = mysql+pymysql://heatuser:heatpass@controller/heat Controller01 Poblamos la base de datos: sudo su -s /bin/sh -c \"heat-manage db_sync\" heat 8.3 Servicios, usuarios y endpoints controller01 Creamos el usuario heat y lo a\u00f1adimos al proyecto service con el rol admin : source adminrc openstack user create --domain default --password h34t heat openstack role add --project service --user heat admin Creamos el servicio heat : openstack service create --name heat --description \"Orchestration\" orchestration Cremos los endpoints para Heat: openstack endpoint create --region SanSebastian orchestration public http://public-api.hispavistaroot.local:8004/v1/%\\(tenant_id\\)s openstack endpoint create --region SanSebastian orchestration internal http://controller:8004/v1/%\\(tenant_id\\)s openstack endpoint create --region SanSebastian orchestration admin http://controller:8004/v1/%\\(tenant_id\\)s Creamos el servicio heat-cfn : openstack service create --name heat-cfn --description \"Orchestration\" cloudformation Creamos los endpoints para Heat CloudFormation: openstack endpoint create --region SanSebastian cloudformation public http://public-api.hispavistaroot.local:8000/v1 openstack endpoint create --region SanSebastian cloudformation internal http://controller:8000/v1 openstack endpoint create --region SanSebastian cloudformation admin http://controller:8000/v1 Creamos el dominio heat : openstack domain create --description \"Stack projects and users\" heat Creamos el usuario heat_domain_admin en el dominio heat y le damos el rol admin : openstack user create --domain heat --password h34t heat_domain_admin openstack role add --domain heat --user-domain heat --user heat_domain_admin admin Creamos el rol heat_stack_owner y se lo damos al usuario hvsistemas en el proyecto hispavista (habr\u00e1 que repetir esto por cada usuario en cada proyecto que queramos que tenga capacidad de orquestar): openstack role create heat_stack_owner openstack role add --project hispavista --user hvsistemas heat_stack_owner Creamos el rol heat_stack_user : openstack role create heat_stack_user 8.4 Configuraci\u00f3n controllers Configuramos Heat editando el fichero /etc/heat/heat.conf : [DEFAULT] bind_host = 172.16.100.1x transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack heat_metadata_server_url = http://controller:8000 heat_waitcondition_server_url = http://controller:8000/v1/waitcondition stack_domain_admin = heat_domain_admin stack_domain_admin_password = h34t stack_user_domain_name = heat [database] connection = mysql+pymysql://heatuser:heatpass@controller/heat [keystone_authtoken] auth_uri = http://public-api.hispavistaroot.local:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = heat password = h34t backend = dogpile.cache.memcached memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [trustee] auth_type = password auth_url = http://controller:35357 username = heat password = h34t user_domain_name = default [clients_keystone] auth_uri = http://controller:35357 [ec2authtoken] auth_uri = http://public-api.hispavistaroot.local:5000/v3 Reiniciamos los servicios: sudo service heat-api restart sudo service heat-api-cfn restart sudo service heat-engine restart 8.5 Comprobaci\u00f3n 8.5.1 Administrador controller01 Comprobamos que est\u00e1n todos los servicios: source adminrc openstack orchestration service list NOTA: CREEMOS que el n\u00famero de servicios de orquestaci\u00f3n creados en cada nodo es proporcional al n\u00famero de n\u00facleos. (COMPROBAR CON UNA M\u00c1QUINA NUEVA) 8.5.2 Usuario local Creamos el fichero template-demo.yml : heat_template_version: 2015-10-15 description: Launch a basic instance with Ubuntu image using the tiny flavor, hvsistemas key and one network parameters: NetID: type: string description: Network ID to use for the instance. resources: server: type: OS::Nova::Server properties: image: \"Ubuntu 16.04 (amd64)\" flavor: \"tiny\" key_name: \"hvsistemas\" networks: - network: { get_param: NetID } outputs: instance_name: description: Name of the instance. value: { get_attr: [ server, name ] } instance_ip: description: IP address of the instance. value: { get_attr: [ server, first_address ] } Lanzamos la plantilla template-demo.yml : source hvsistemasrc export NET_ID=internal openstack stack create -t template-demo.yml --parameter \"NetID=$NET_ID\" mystack Listamos los stacks hasta que est\u00e9 completo: openstack stack list Vemos la salida de la ejecuci\u00f3n del stack: openstack stack output show --all mystack Listamos las instancias: openstack server list Borramos el stack: openstack stack delete --yes mystack Lanzamos el stack mylamp desde el fichero template-lamp.yml : openstack stack create -t template-lamp.yml mylamp Listamos los stacks hasta que est\u00e9 completo: openstack stack list Vemos la salida de la ejecuci\u00f3n del stack: openstack stack output show --all mylamp openstack stack output show mylamp deploy_ip -c output_value -f value","title":"8. - Heat"},{"location":"openstack/08-heat/#8-heat","text":"","title":"8. Heat"},{"location":"openstack/08-heat/#81-instalacion","text":"controllers Instalamos los paquetes para Heat: sudo apt install heat-api heat-api-cfn heat-engine","title":"8.1 Instalaci\u00f3n"},{"location":"openstack/08-heat/#82-base-de-datos","text":"controller01 Creamos la base de datos para Heat: sudo mysql -p CREATE DATABASE heat; GRANT ALL PRIVILEGES ON heat.* TO 'heatuser'@'localhost' IDENTIFIED BY 'heatpass'; GRANT ALL PRIVILEGES ON heat.* TO 'heatuser'@'%' IDENTIFIED BY 'heatpass'; Controllers Editamos el fichero /etc/heat/heat.conf : [database] connection = mysql+pymysql://heatuser:heatpass@controller/heat Controller01 Poblamos la base de datos: sudo su -s /bin/sh -c \"heat-manage db_sync\" heat","title":"8.2 Base de datos"},{"location":"openstack/08-heat/#83-servicios-usuarios-y-endpoints","text":"controller01 Creamos el usuario heat y lo a\u00f1adimos al proyecto service con el rol admin : source adminrc openstack user create --domain default --password h34t heat openstack role add --project service --user heat admin Creamos el servicio heat : openstack service create --name heat --description \"Orchestration\" orchestration Cremos los endpoints para Heat: openstack endpoint create --region SanSebastian orchestration public http://public-api.hispavistaroot.local:8004/v1/%\\(tenant_id\\)s openstack endpoint create --region SanSebastian orchestration internal http://controller:8004/v1/%\\(tenant_id\\)s openstack endpoint create --region SanSebastian orchestration admin http://controller:8004/v1/%\\(tenant_id\\)s Creamos el servicio heat-cfn : openstack service create --name heat-cfn --description \"Orchestration\" cloudformation Creamos los endpoints para Heat CloudFormation: openstack endpoint create --region SanSebastian cloudformation public http://public-api.hispavistaroot.local:8000/v1 openstack endpoint create --region SanSebastian cloudformation internal http://controller:8000/v1 openstack endpoint create --region SanSebastian cloudformation admin http://controller:8000/v1 Creamos el dominio heat : openstack domain create --description \"Stack projects and users\" heat Creamos el usuario heat_domain_admin en el dominio heat y le damos el rol admin : openstack user create --domain heat --password h34t heat_domain_admin openstack role add --domain heat --user-domain heat --user heat_domain_admin admin Creamos el rol heat_stack_owner y se lo damos al usuario hvsistemas en el proyecto hispavista (habr\u00e1 que repetir esto por cada usuario en cada proyecto que queramos que tenga capacidad de orquestar): openstack role create heat_stack_owner openstack role add --project hispavista --user hvsistemas heat_stack_owner Creamos el rol heat_stack_user : openstack role create heat_stack_user","title":"8.3 Servicios, usuarios y endpoints"},{"location":"openstack/08-heat/#84-configuracion","text":"controllers Configuramos Heat editando el fichero /etc/heat/heat.conf : [DEFAULT] bind_host = 172.16.100.1x transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack heat_metadata_server_url = http://controller:8000 heat_waitcondition_server_url = http://controller:8000/v1/waitcondition stack_domain_admin = heat_domain_admin stack_domain_admin_password = h34t stack_user_domain_name = heat [database] connection = mysql+pymysql://heatuser:heatpass@controller/heat [keystone_authtoken] auth_uri = http://public-api.hispavistaroot.local:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = heat password = h34t backend = dogpile.cache.memcached memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [trustee] auth_type = password auth_url = http://controller:35357 username = heat password = h34t user_domain_name = default [clients_keystone] auth_uri = http://controller:35357 [ec2authtoken] auth_uri = http://public-api.hispavistaroot.local:5000/v3 Reiniciamos los servicios: sudo service heat-api restart sudo service heat-api-cfn restart sudo service heat-engine restart","title":"8.4 Configuraci\u00f3n"},{"location":"openstack/08-heat/#85-comprobacion","text":"","title":"8.5 Comprobaci\u00f3n"},{"location":"openstack/08-heat/#851-administrador","text":"controller01 Comprobamos que est\u00e1n todos los servicios: source adminrc openstack orchestration service list NOTA: CREEMOS que el n\u00famero de servicios de orquestaci\u00f3n creados en cada nodo es proporcional al n\u00famero de n\u00facleos. (COMPROBAR CON UNA M\u00c1QUINA NUEVA)","title":"8.5.1 Administrador"},{"location":"openstack/08-heat/#852-usuario","text":"local Creamos el fichero template-demo.yml : heat_template_version: 2015-10-15 description: Launch a basic instance with Ubuntu image using the tiny flavor, hvsistemas key and one network parameters: NetID: type: string description: Network ID to use for the instance. resources: server: type: OS::Nova::Server properties: image: \"Ubuntu 16.04 (amd64)\" flavor: \"tiny\" key_name: \"hvsistemas\" networks: - network: { get_param: NetID } outputs: instance_name: description: Name of the instance. value: { get_attr: [ server, name ] } instance_ip: description: IP address of the instance. value: { get_attr: [ server, first_address ] } Lanzamos la plantilla template-demo.yml : source hvsistemasrc export NET_ID=internal openstack stack create -t template-demo.yml --parameter \"NetID=$NET_ID\" mystack Listamos los stacks hasta que est\u00e9 completo: openstack stack list Vemos la salida de la ejecuci\u00f3n del stack: openstack stack output show --all mystack Listamos las instancias: openstack server list Borramos el stack: openstack stack delete --yes mystack Lanzamos el stack mylamp desde el fichero template-lamp.yml : openstack stack create -t template-lamp.yml mylamp Listamos los stacks hasta que est\u00e9 completo: openstack stack list Vemos la salida de la ejecuci\u00f3n del stack: openstack stack output show --all mylamp openstack stack output show mylamp deploy_ip -c output_value -f value","title":"8.5.2 Usuario"},{"location":"openstack/09-ceilometer/","text":"9. CEILOMETER 9.1 Instalaci\u00f3n KVM01 y KVM02 Instalamos paquetes de Ceilometer: sudo apt-get install ceilometer-agent-compute Editamos el fichero /etc/ceilometer/ceilometer.conf : [DEFAULT] transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = ceilometer password = c31l0m3t3r backend = dogpile.cache.memcached memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [service_credentials] auth_url = http://controller:5000 auth_type = password project_domain_id = default user_domain_id = default project_name = service username = ceilometer password = c31l0m3t3r interface = internalURL Editamos el fichero /etc/nova/nova.conf : [DEFAULT] ... instance_usage_audit = True instance_usage_audit_period = hour notify_on_state_change = vm_and_task_state [oslo_messaging_notifications] ... driver = messagingv2 Reiniciamos servicios: sudo service ceilometer-agent-compute restart sudo service nova-compute restart Controllers Usamos las credenciales de admin y creamos el usuario ceilometer y lo a\u00f1adimos al proyecto service con el rol admin: . adminrc openstack user create --domain default --password c31l0m3t3r ceilometer openstack role add --project service --user ceilometer admin Creamos el servicio ceilometer: openstack service create --name ceilometer --description \"Telemetry\" metering Registramos a gnocchi en Keystone y a\u00f1adimos creamos su servicio: openstack user create --domain default --password gn0cch1 gnocchi openstack role add --project service --user gnocchi admin openstack service create --name gnocchi --description \"Metric Service\" metric Creamos los endpoints para el servicio gnocchi: openstack endpoint create --region SanSebastian metric public http://public-api.hispavistaroot.local:8041 openstack endpoint create --region SanSebastian metric internal http://controller:8041 openstack endpoint create --region SanSebastian metric admin http://controller:8041 Instalamos los paquetes de Gnocchi: sudo apt install gnocchi-api gnocchi-metricd python-gnocchiclient (dpkg-reconfigure -plow gnocchi-common) Creamos la base de datos y el usuario para gnocchi: sudo mysql -p CREATE DATABASE gnocchi; GRANT ALL PRIVILEGES ON gnocchi.* TO 'gnocchi'@'localhost' IDENTIFIED BY 'gn0cch1'; GRANT ALL PRIVILEGES ON gnocchi.* TO 'gnocchi'@'%' IDENTIFIED BY 'gn0cch1'; Editamos el fichero /etc/gnocchi/gnocchi.conf : [api] auth_mode = keystone [keystone_authtoken] ... auth_type = password auth_uri = http://public-api.hispavistaroot.local:5000 auth_url = http://controller:35357 project_domain_name = Default user_domain_name = Default project_name = service username = gnocchi password = gn0cch1 interface = internalURL region_name = SanSebastian backend = dogpile.cache.memcached memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [indexer] url = mysql+pymysql://gnocchi:gn0cch1@controller/gnocchi [storage] coordination_url = redis://controller:6379 file_basepath = /var/lib/gnocchi driver = file Instalamos Redis: sudo apt-get install redis-server (Tambi\u00e9n se puede instalar mongo como bbdd: https://docs.openstack.org/mitaka/install-guide-ubuntu/environment-nosql-database.html) Inicializamos gnocchi y reiniciamos servicios: sudo gnocchi-upgrade ##############sudo service gnocchi-api restart sudo service gnocchi-metricd restart sudo service apache2 restart Instalamos componentes: sudo apt install ceilometer-agent-notification ceilometer-agent-central Editamos /etc/ceilometer/ceilometer.conf : [DEFAULT] transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack [dispatcher_gnocchi] # filter out Gnocchi-related activity meters (Swift driver) filter_service_activity = False # default metric storage archival policy archive_policy = low [service_credentials] auth_url = http://controller:5000 auth_type = password project_domain_id = default user_domain_id = default project_name = service username = ceilometer password = c31l0m3t3r interface = internalURL [keystone_authtoken] auth_uri = http://public-api.hispavistaroot.local:5000/v3 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = ceilometer password = c31l0m3t3r backend = dogpile.cache.memcached memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [compute] instance_discovery_method = libvirt_metadata A\u00f1adir lo siguiente a /etc/apache2/sites-enabled/gnocchi-api.conf : <Directory /> Options FollowSymLinks AllowOverride None <IfVersion >= 2.4> Require all granted </IfVersion> <IfVersion < 2.4> Order allow,deny Allow from all </IfVersion> </Directory> Creamos los recursos de Gnocchi y reiniciamos servicios: service apache2 restart sudo ceilometer-upgrade --skip-metering-database ### Este comando s\u00f3lo en un Controller sudo service ceilometer-agent-central restart sudo service ceilometer-agent-notification restart 9.2 Configuraci\u00f3n en servicios Storage Editamos /etc/cinder/cinder.conf : [oslo_messaging_notifications] ... driver = messagingv2 Probamos que las estad\u00edsiticas peri\u00f3dicas funcionen (rellenando las horas y fechas): cinder-volume-usage-audit --start_time='YYYY-MM-DD HH:MM:SS' \\ --end_time='YYYY-MM-DD HH:MM:SS' --send_actions A\u00f1adimos esto al cron /etc/cron.d/cinder-stats : */5 * * * * /usr/bin/cinder-volume-usage-audit --send_actions Reiniciamos servicios en el Controller : sudo service apache2 restart sudo service cinder-scheduler restart Y en Storage : sudo service cinder-volume restart Glance - Controller Editamos /etc/glance/glance-api.conf y /etc/glance/glance-registry.conf : [DEFAULT] transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack [oslo_messaging_notifications] driver = messagingv2 Reiniciamos los servicios: sudo service glance-registry restart sudo service glance-api restart Heat - Controller Editamos /etc/heat/heat.conf : [oslo_messaging_notifications] driver = messagingv2 Reiniciamos servicios: sudo service heat-api restart sudo service heat-api-cfn restart sudo service heat-engine restart Neutron - Controller Editamos /etc/neutron/neutron.conf : [oslo_messaging_notifications] driver = messagingv2 Reiniciamos el servicio: sudo service neutron-server restart 9.3 - Comprobaciones para comprobar que todo funciona hay que hacer lo siguiente: . adminrc export OS_AUTH_TYPE=password gnocchi resource list --type image gnocchi resource show a6b387e1-4276-43db-b17a-e10f649d85a3 Si queremos usar grafana En los ficheros keystone.conf y gnocchi.conf a\u00f1adimos lo siguiente: [cors] allowed_origin = http://ruta-a-grafana:puerto","title":"9. - Ceilometer"},{"location":"openstack/09-ceilometer/#9-ceilometer","text":"","title":"9. CEILOMETER"},{"location":"openstack/09-ceilometer/#91-instalacion","text":"KVM01 y KVM02 Instalamos paquetes de Ceilometer: sudo apt-get install ceilometer-agent-compute Editamos el fichero /etc/ceilometer/ceilometer.conf : [DEFAULT] transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = ceilometer password = c31l0m3t3r backend = dogpile.cache.memcached memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [service_credentials] auth_url = http://controller:5000 auth_type = password project_domain_id = default user_domain_id = default project_name = service username = ceilometer password = c31l0m3t3r interface = internalURL Editamos el fichero /etc/nova/nova.conf : [DEFAULT] ... instance_usage_audit = True instance_usage_audit_period = hour notify_on_state_change = vm_and_task_state [oslo_messaging_notifications] ... driver = messagingv2 Reiniciamos servicios: sudo service ceilometer-agent-compute restart sudo service nova-compute restart Controllers Usamos las credenciales de admin y creamos el usuario ceilometer y lo a\u00f1adimos al proyecto service con el rol admin: . adminrc openstack user create --domain default --password c31l0m3t3r ceilometer openstack role add --project service --user ceilometer admin Creamos el servicio ceilometer: openstack service create --name ceilometer --description \"Telemetry\" metering Registramos a gnocchi en Keystone y a\u00f1adimos creamos su servicio: openstack user create --domain default --password gn0cch1 gnocchi openstack role add --project service --user gnocchi admin openstack service create --name gnocchi --description \"Metric Service\" metric Creamos los endpoints para el servicio gnocchi: openstack endpoint create --region SanSebastian metric public http://public-api.hispavistaroot.local:8041 openstack endpoint create --region SanSebastian metric internal http://controller:8041 openstack endpoint create --region SanSebastian metric admin http://controller:8041 Instalamos los paquetes de Gnocchi: sudo apt install gnocchi-api gnocchi-metricd python-gnocchiclient (dpkg-reconfigure -plow gnocchi-common) Creamos la base de datos y el usuario para gnocchi: sudo mysql -p CREATE DATABASE gnocchi; GRANT ALL PRIVILEGES ON gnocchi.* TO 'gnocchi'@'localhost' IDENTIFIED BY 'gn0cch1'; GRANT ALL PRIVILEGES ON gnocchi.* TO 'gnocchi'@'%' IDENTIFIED BY 'gn0cch1'; Editamos el fichero /etc/gnocchi/gnocchi.conf : [api] auth_mode = keystone [keystone_authtoken] ... auth_type = password auth_uri = http://public-api.hispavistaroot.local:5000 auth_url = http://controller:35357 project_domain_name = Default user_domain_name = Default project_name = service username = gnocchi password = gn0cch1 interface = internalURL region_name = SanSebastian backend = dogpile.cache.memcached memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [indexer] url = mysql+pymysql://gnocchi:gn0cch1@controller/gnocchi [storage] coordination_url = redis://controller:6379 file_basepath = /var/lib/gnocchi driver = file Instalamos Redis: sudo apt-get install redis-server (Tambi\u00e9n se puede instalar mongo como bbdd: https://docs.openstack.org/mitaka/install-guide-ubuntu/environment-nosql-database.html) Inicializamos gnocchi y reiniciamos servicios: sudo gnocchi-upgrade ##############sudo service gnocchi-api restart sudo service gnocchi-metricd restart sudo service apache2 restart Instalamos componentes: sudo apt install ceilometer-agent-notification ceilometer-agent-central Editamos /etc/ceilometer/ceilometer.conf : [DEFAULT] transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack [dispatcher_gnocchi] # filter out Gnocchi-related activity meters (Swift driver) filter_service_activity = False # default metric storage archival policy archive_policy = low [service_credentials] auth_url = http://controller:5000 auth_type = password project_domain_id = default user_domain_id = default project_name = service username = ceilometer password = c31l0m3t3r interface = internalURL [keystone_authtoken] auth_uri = http://public-api.hispavistaroot.local:5000/v3 auth_url = http://controller:35357 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = ceilometer password = c31l0m3t3r backend = dogpile.cache.memcached memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 [compute] instance_discovery_method = libvirt_metadata A\u00f1adir lo siguiente a /etc/apache2/sites-enabled/gnocchi-api.conf : <Directory /> Options FollowSymLinks AllowOverride None <IfVersion >= 2.4> Require all granted </IfVersion> <IfVersion < 2.4> Order allow,deny Allow from all </IfVersion> </Directory> Creamos los recursos de Gnocchi y reiniciamos servicios: service apache2 restart sudo ceilometer-upgrade --skip-metering-database ### Este comando s\u00f3lo en un Controller sudo service ceilometer-agent-central restart sudo service ceilometer-agent-notification restart","title":"9.1 Instalaci\u00f3n"},{"location":"openstack/09-ceilometer/#92-configuracion-en-servicios","text":"Storage Editamos /etc/cinder/cinder.conf : [oslo_messaging_notifications] ... driver = messagingv2 Probamos que las estad\u00edsiticas peri\u00f3dicas funcionen (rellenando las horas y fechas): cinder-volume-usage-audit --start_time='YYYY-MM-DD HH:MM:SS' \\ --end_time='YYYY-MM-DD HH:MM:SS' --send_actions A\u00f1adimos esto al cron /etc/cron.d/cinder-stats : */5 * * * * /usr/bin/cinder-volume-usage-audit --send_actions Reiniciamos servicios en el Controller : sudo service apache2 restart sudo service cinder-scheduler restart Y en Storage : sudo service cinder-volume restart Glance - Controller Editamos /etc/glance/glance-api.conf y /etc/glance/glance-registry.conf : [DEFAULT] transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack [oslo_messaging_notifications] driver = messagingv2 Reiniciamos los servicios: sudo service glance-registry restart sudo service glance-api restart Heat - Controller Editamos /etc/heat/heat.conf : [oslo_messaging_notifications] driver = messagingv2 Reiniciamos servicios: sudo service heat-api restart sudo service heat-api-cfn restart sudo service heat-engine restart Neutron - Controller Editamos /etc/neutron/neutron.conf : [oslo_messaging_notifications] driver = messagingv2 Reiniciamos el servicio: sudo service neutron-server restart","title":"9.2 Configuraci\u00f3n en servicios"},{"location":"openstack/09-ceilometer/#93-comprobaciones","text":"para comprobar que todo funciona hay que hacer lo siguiente: . adminrc export OS_AUTH_TYPE=password gnocchi resource list --type image gnocchi resource show a6b387e1-4276-43db-b17a-e10f649d85a3","title":"9.3 - Comprobaciones"},{"location":"openstack/09-ceilometer/#si-queremos-usar-grafana","text":"En los ficheros keystone.conf y gnocchi.conf a\u00f1adimos lo siguiente: [cors] allowed_origin = http://ruta-a-grafana:puerto","title":"Si queremos usar grafana"},{"location":"previos/00-1-HA - Corosync y Pacemaker/","text":"COROSYNC Y PACEMAKER 1. Instalaci\u00f3n Controller01 y Controller02 (en todos los controllers) Instalar los siguientes paquetes: apt install pacemaker corosync pcs fence-agents resource-agents libqb0 2. Corosync Habilitamos el servicio y lo a\u00f1adimos al arranque: update-rc.d corosync defaults update-rc.d corosync enable Movemos el fichero de configuraci\u00f3n original: mv /etc/corosync/corosync.conf /etc/corosync/corosync.conf.bak Editamos el fichero /etc/corosync/corosync.conf`: totem { version: 2 secauth: off cluster_name: openstackv1 transport: udpu } nodelist { node { ring0_addr: controller01 nodeid: 1 } node { ring0_addr: controller02 nodeid: 2 } } quorum { provider: corosync_votequorum two_node: 1 } logging { to_logfile: yes logfile: /var/log/corosync/corosync.log to_syslog: yes } NOTA: tal vez deber\u00edamos a\u00f1adir otra interfaz de red aqu\u00ed, para que el anillo funcione mejor. Ejecutamoms mkdir -p /etc/corosync/uidgid.d/ y creamos el fichero /etc/corosync/uidgid.d/pacemaker : uidgid { uid: hacluster gid: haclient } 3. Configuraci\u00f3n IMPORTANTE: asegurarse de que en /etc/hosts NO hay referencias al nombre de la m\u00e1quina con IPs locales, como por ejemplo: 127.0.1.1 controller01 ::1 localhost ip6-localhost ip6-loopback Controller01 y Controller02 (en todos los controllers) Asegurarse de que pcsd se inicia como servicio y establecer una contrase\u00f1a al usuario hacluster : passwd hacluster systemctl enable pcsd systemctl start pcsd En uno de los controllers Autenticamos todos los miembros del cluster: pcs cluster auth controller01 controller02 -u hacluster -p CONTRASE\u00d1A --force NOTA: controller01p y controller02p son las ip's p\u00fablicas Creamos y damos nombre al cluster, lo incializamos y habilitamos el autoarranque: pcs cluster setup --force --name openstackv1 controller01 controller02 pcs cluster start --all pcs cluster enable --all En caso de que el comando setup nos hubiera devuelto un error, ejecutar pcs cluster auth , meter el usuario hacluster y la contrase\u00f1a que hemos puesto antes y nos saldr\u00e1 algo as\u00ed: Username: hacluster Password: controller02p: Authorized controller01p: Authorized 4. Comprobaci\u00f3n de estado Podemos usar los siguientes comandos para ver el estado del cluster: corosync-cmapctl | grep members corosync-cfgtool -s pcs status cluster El resultado del \u00faltimo, si todo va bien ser\u00eda algo as\u00ed: Cluster Status: Last updated: Wed Apr 11 10:42:26 2018 Last change: Wed Apr 11 10:40:43 2018 by hacluster via crmd on controller01p Stack: corosync Current DC: controller01p (version 1.1.14-70404b0) - partition with quorum 2 nodes and 0 resources configured Online: [ controller01p controller02p ] PCSD Status: controller01p: Online controller02p: Online 5. Configuraci\u00f3n de propiedades del cluster En uno de los controllers pcs property set pe-warn-series-max=1000 \\ pe-input-series-max=1000 \\ pe-error-series-max=1000 \\ cluster-recheck-interval=5min En el entorno de desarrollo desactivaremos los mecanismos STONITH (apagar nodos v\u00eda IPMI o ssh cuando fallen), pero en producci\u00f3n hay que dejarlo activado, siempre y cuando tengamos m\u00ednimo 3 nodos. Tambi\u00e9n desactivaremos el quorum, ya que con dos nodos no tiene sentido: pcs property set stonith-enabled=false pcs property set no-quorum-policy=ignore 5. A\u00f1adir IP de balanceo Para a\u00f1adir una Virtual IP escribimos lo siguiente: pcs resource create vip ocf:heartbeat:IPaddr2 \\ params ip=\"172.16.100.10\" cidr_netmask=\"24\" op monitor interval=\"30s\" pcs resource create vip2 ocf:heartbeat:IPaddr2 \\ params ip=\"192.168.111.10\" cidr_netmask=\"24\" op monitor interval=\"30s\" NOTA: podemos sustituir la IP 172.16.100.10 por 192.168.111.10 si queremos hacer el balanceo hacia fuera. Para borrar la VIP escribimos pcs resource delete vip","title":"0.1 - Corosync y Pacemaker"},{"location":"previos/00-1-HA - Corosync y Pacemaker/#corosync-y-pacemaker","text":"","title":"COROSYNC Y PACEMAKER"},{"location":"previos/00-1-HA - Corosync y Pacemaker/#1-instalacion","text":"Controller01 y Controller02 (en todos los controllers) Instalar los siguientes paquetes: apt install pacemaker corosync pcs fence-agents resource-agents libqb0","title":"1. Instalaci\u00f3n"},{"location":"previos/00-1-HA - Corosync y Pacemaker/#2-corosync","text":"Habilitamos el servicio y lo a\u00f1adimos al arranque: update-rc.d corosync defaults update-rc.d corosync enable Movemos el fichero de configuraci\u00f3n original: mv /etc/corosync/corosync.conf /etc/corosync/corosync.conf.bak Editamos el fichero /etc/corosync/corosync.conf`: totem { version: 2 secauth: off cluster_name: openstackv1 transport: udpu } nodelist { node { ring0_addr: controller01 nodeid: 1 } node { ring0_addr: controller02 nodeid: 2 } } quorum { provider: corosync_votequorum two_node: 1 } logging { to_logfile: yes logfile: /var/log/corosync/corosync.log to_syslog: yes } NOTA: tal vez deber\u00edamos a\u00f1adir otra interfaz de red aqu\u00ed, para que el anillo funcione mejor. Ejecutamoms mkdir -p /etc/corosync/uidgid.d/ y creamos el fichero /etc/corosync/uidgid.d/pacemaker : uidgid { uid: hacluster gid: haclient }","title":"2. Corosync"},{"location":"previos/00-1-HA - Corosync y Pacemaker/#3-configuracion","text":"","title":"3. Configuraci\u00f3n"},{"location":"previos/00-1-HA - Corosync y Pacemaker/#importante-asegurarse-de-que-en-etchosts-no-hay-referencias-al-nombre-de-la-maquina-con-ips-locales-como-por-ejemplo","text":"127.0.1.1 controller01 ::1 localhost ip6-localhost ip6-loopback Controller01 y Controller02 (en todos los controllers) Asegurarse de que pcsd se inicia como servicio y establecer una contrase\u00f1a al usuario hacluster : passwd hacluster systemctl enable pcsd systemctl start pcsd En uno de los controllers Autenticamos todos los miembros del cluster: pcs cluster auth controller01 controller02 -u hacluster -p CONTRASE\u00d1A --force NOTA: controller01p y controller02p son las ip's p\u00fablicas Creamos y damos nombre al cluster, lo incializamos y habilitamos el autoarranque: pcs cluster setup --force --name openstackv1 controller01 controller02 pcs cluster start --all pcs cluster enable --all En caso de que el comando setup nos hubiera devuelto un error, ejecutar pcs cluster auth , meter el usuario hacluster y la contrase\u00f1a que hemos puesto antes y nos saldr\u00e1 algo as\u00ed: Username: hacluster Password: controller02p: Authorized controller01p: Authorized","title":"IMPORTANTE: asegurarse de que en /etc/hosts NO hay referencias al nombre de la m\u00e1quina con IPs locales, como por ejemplo:"},{"location":"previos/00-1-HA - Corosync y Pacemaker/#4-comprobacion-de-estado","text":"Podemos usar los siguientes comandos para ver el estado del cluster: corosync-cmapctl | grep members corosync-cfgtool -s pcs status cluster El resultado del \u00faltimo, si todo va bien ser\u00eda algo as\u00ed: Cluster Status: Last updated: Wed Apr 11 10:42:26 2018 Last change: Wed Apr 11 10:40:43 2018 by hacluster via crmd on controller01p Stack: corosync Current DC: controller01p (version 1.1.14-70404b0) - partition with quorum 2 nodes and 0 resources configured Online: [ controller01p controller02p ] PCSD Status: controller01p: Online controller02p: Online","title":"4. Comprobaci\u00f3n de estado"},{"location":"previos/00-1-HA - Corosync y Pacemaker/#5-configuracion-de-propiedades-del-cluster","text":"En uno de los controllers pcs property set pe-warn-series-max=1000 \\ pe-input-series-max=1000 \\ pe-error-series-max=1000 \\ cluster-recheck-interval=5min En el entorno de desarrollo desactivaremos los mecanismos STONITH (apagar nodos v\u00eda IPMI o ssh cuando fallen), pero en producci\u00f3n hay que dejarlo activado, siempre y cuando tengamos m\u00ednimo 3 nodos. Tambi\u00e9n desactivaremos el quorum, ya que con dos nodos no tiene sentido: pcs property set stonith-enabled=false pcs property set no-quorum-policy=ignore","title":"5. Configuraci\u00f3n de propiedades del cluster"},{"location":"previos/00-1-HA - Corosync y Pacemaker/#5-anadir-ip-de-balanceo","text":"Para a\u00f1adir una Virtual IP escribimos lo siguiente: pcs resource create vip ocf:heartbeat:IPaddr2 \\ params ip=\"172.16.100.10\" cidr_netmask=\"24\" op monitor interval=\"30s\" pcs resource create vip2 ocf:heartbeat:IPaddr2 \\ params ip=\"192.168.111.10\" cidr_netmask=\"24\" op monitor interval=\"30s\" NOTA: podemos sustituir la IP 172.16.100.10 por 192.168.111.10 si queremos hacer el balanceo hacia fuera. Para borrar la VIP escribimos pcs resource delete vip","title":"5. A\u00f1adir IP de balanceo"},{"location":"previos/00-2-HA - HAPROXY/","text":"HAPROXY 1. Instalaci\u00f3n Controllers apt install haproxy hatop 2. Configuraci\u00f3n 2.1. Rsyslog Para que HAProxy pueda logear eventos en un fichero local, tenemos que hacer un cambio en /etc/rsyslog.d/49-haproxy.conf . Deber\u00eda quedarnos as\u00ed: # Create an additional socket in haproxy's chroot in order to allow logging via # /dev/log to chroot'ed HAProxy processes #$AddUnixListenSocket /var/lib/haproxy/dev/log # # Send HAProxy messages to a dedicated logfile #if $programname startswith 'haproxy' then /var/log/haproxy.log #&~ # .. otherwise consider putting these two in /etc/rsyslog.conf instead: $ModLoad imudp $UDPServerAddress 127.0.0.1 $UDPServerRun 514 # ..and in any case, put these two in /etc/rsyslog.d/49-haproxy.conf: local0.* -/var/log/haproxy_0.log & ~ # & ~ means not to put what matched in the above line anywhere else for the rest of the rules # http://serverfault.com/questions/214312/how-to-keep-haproxy-log-messages-out-of-var-log-syslog Y reiniciamos el servicio: service rsyslog restart 2.2. HAProxy IMPORTANTE: user haproxy en el chequeo de Galera, creado en la BBDD con un create user 'haproxy'@'172.16.100.%'; Hay que balancear todos los servicios, as\u00ed que el fichero /etc/haproxy/haproxy.cfg quedar\u00eda as\u00ed, m\u00e1s o menos: global log 127.0.0.1 local0 debug # log /var/lib/haproxy/dev/log local0 debug chroot /var/lib/haproxy daemon group haproxy maxconn 4000 pidfile /var/run/haproxy.pid user haproxy stats socket /var/run/haproxy.sock mode 600 level admin stats timeout 2m defaults log global maxconn 4000 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout check 10s listen rabbit_cluster # bind 172.16.100.10:5672 # balance source option tcpka # option httpchk option tcplog server controller01 172.16.100.11:5672 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:5672 check inter 2000 rise 2 fall 5 # server controller03p 192.168.111.13:5672 check inter 2000 rise 2 fall 5 listen dashboard_cluster bind 192.168.111.10:80 balance source option tcpka option httpchk option tcplog server controller01p 192.168.111.11:80 check inter 2000 rise 2 fall 5 server controller02p 192.168.111.12:80 check inter 2000 rise 2 fall 5 # server controller03p 192.168.111.13:80 check inter 2000 rise 2 fall 5 listen galera_cluster bind 192.168.111.10:3306 bind 172.16.100.10:3306 balance source option mysql-check user haproxy server controller01 172.16.100.11:3306 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:3306 backup check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:3306 backup check inter 2000 rise 2 fall 5 listen glance_api_cluster bind 172.16.100.10:9292 bind 192.168.111.10:9292 balance source option tcpka option httpchk option tcplog server controller01 172.16.100.11:9292 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:9292 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:9292 check inter 2000 rise 2 fall 5 listen glance_registry_cluster bind 172.16.100.10:9191 bind 192.168.111.10:9191 balance source option tcpka option tcplog server controller01 172.16.100.11:9191 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:9191 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:9191 check inter 2000 rise 2 fall 5 # listen keystone_admin_cluster bind 172.16.100.10:35357 balance source option tcpka option httpchk option tcplog server controller01 172.16.100.11:35357 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:35357 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:35357 check inter 2000 rise 2 fall 5 listen keystone_public_internal_cluster bind 192.168.111.10:5000 balance source option tcpka option httpchk option tcplog server controller01p 192.168.111.11:5000 check inter 2000 rise 2 fall 5 server controller02p 192.168.111.12:5000 check inter 2000 rise 2 fall 5 # server controller03p 192.168.111.13:5000 check inter 2000 rise 2 fall 5 listen keystone_internal_cluster bind 172.16.100.10:5000 balance source option tcpka option httpchk option tcplog server controller01 172.16.100.11:5000 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:5000 check inter 2000 rise 2 fall 5 # server controller03p 192.168.111.13:5000 check inter 2000 rise 2 fall 5 # listen nova_ec2_api_cluster # bind 172.16.100.10:8773 # balance source # option tcpka # option tcplog # server controller01 172.16.100.11:8773 check inter 2000 rise 2 fall 5 # server controller02 172.16.100.12:8773 check inter 2000 rise 2 fall 5 ## server controller03 172.16.100.13:8773 check inter 2000 rise 2 fall 5 listen nova_compute_api_cluster bind 172.16.100.10:8774 bind 192.168.111.10:8774 balance source option tcpka option httpchk option tcplog server controller01 172.16.100.11:8774 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:8774 check inter 2000 rise 2 fall 5 ## server controller03 172.16.100.13:8774 check inter 2000 rise 2 fall 5 # listen nova_metadata_api_cluster bind 172.16.100.10:8775 bind 192.168.111.10:8775 balance source option tcpka option tcplog server controller01 172.16.100.11:8775 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:8775 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:8775 check inter 2000 rise 2 fall 5 listen nova_placement_api_cluster bind 172.16.100.10:8778 bind 192.168.111.10:8778 balance source option tcpka option tcplog server controller01 172.16.100.11:8778 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:8778 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:8778 check inter 2000 rise 2 fall 5 listen cinder_api_cluster bind 172.16.100.10:8776 bind 192.168.111.10:8776 balance source option tcpka option httpchk option tcplog server controller01 172.16.100.11:8776 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:8776 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:8776 check inter 2000 rise 2 fall 5 listen heat_api_cfn_cluster bind 172.16.100.10:8000 bind 192.168.111.10:8000 balance source option tcpka option httpchk option tcplog server controller01 172.16.100.11:8000 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:8000 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:8000 check inter 2000 rise 2 fall 5 listen heat_api_cluster bind 172.16.100.10:8004 bind 192.168.111.10:8004 balance source option tcpka option httpchk option tcplog server controller01 172.16.100.11:8004 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:8004 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:8004 check inter 2000 rise 2 fall 5 listen gnocchi_api_cluster bind 172.16.100.10:8041 bind 192.168.111.10:8041 balance source option tcpka option httpchk option tcplog server controller01 172.16.100.11:8041 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:8041 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:8041 check inter 2000 rise 2 fall 5 # listen ceilometer_api_cluster # bind 172.16.100.10:8777 # balance source # option tcpka # option tcplog # server controller01 172.16.100.11:8777 check inter 2000 rise 2 fall 5 # server controller02 172.16.100.12:8777 check inter 2000 rise 2 fall 5 ## server controller03 172.16.100.13:8777 check inter 2000 rise 2 fall 5 listen nova_vncproxy_cluster bind 172.16.100.10:6080 bind 192.168.111.10:6080 balance source option tcpka option tcplog server controller01 172.16.100.11:6080 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:6080 check inter 2000 rise 2 fall 5 ## server controller03 172.16.100.13:6080 check inter 2000 rise 2 fall 5 # listen neutron_api_cluster bind 172.16.100.10:9696 bind 192.168.111.10:9696 balance source option tcpka option httpchk option tcplog server controller01 172.16.100.11:9696 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:9696 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:9696 check inter 2000 rise 2 fall 5 Tambi\u00e9n hay que a\u00f1adir lo siguiente al fichero /etc/sysctl.conf para que los controllers puedan escuchar en interfaces de red externas (IP's virtuales): net.ipv4.ip_nonlocal_bind = 1 Y aplicar los cambios ejecutando sysctl -p EN UN CONTROLLER Despu\u00e9s hay que a\u00f1adir los recursos y restricciones a Pacemaker: pcs resource create lb-haproxy lsb:haproxy --clone pcs constraint order start vip then vip2 pcs constraint order start vip2 then lb-haproxy-clone pcs constraint colocation add vip with vip2 pcs constraint colocation add lb-haproxy-clone with vip pcs status Cluster name: openstackv1 Last updated: Wed Apr 18 10:37:00 2018 Last change: Wed Apr 18 10:26:51 2018 by root via cibadmin on controller01 Stack: corosync Current DC: controller02 (version 1.1.14-70404b0) - partition with quorum 2 nodes and 4 resources configured Online: [ controller01 controller02 ] Full list of resources: vip (ocf::heartbeat:IPaddr2): Started controller01 Clone Set: lb-haproxy-clone [lb-haproxy] Started: [ controller01 ] Stopped: [ controller02 ] vip2 (ocf::heartbeat:IPaddr2): Started controller01 PCSD Status: controller01: Online controller02: Online Daemon Status: corosync: active/enabled pacemaker: active/enabled pcsd: active/enabled 3. Comprobaci\u00f3n adicional para Galera Controller01 y Controller02 apt install xinetd Crear el fichero /etc/default/clusterchek : MYSQL_USERNAME=\"clustercheck_user\" MYSQL_PASSWORD=\"my_clustercheck_password\" MYSQL_HOST=\"localhost\" MYSQL_PORT=\"3306\" A\u00f1adir el usuario a la base de datos: GRANT PROCESS ON *.* TO 'clustercheck_user'@'localhost' IDENTIFIED BY 'my_clustercheck_password'; GRANT PROCESS ON *.* TO 'clustercheck_user'@'controller01' IDENTIFIED BY 'my_clustercheck_password'; GRANT PROCESS ON *.* TO 'clustercheck_user'@'controller02' IDENTIFIED BY 'my_clustercheck_password'; FLUSH PRIVILEGES; Crear el fichero /etc/xinetd.d/galera-monitor : service galera-monitor { port = 9200 disable = no socket_type = stream protocol = tcp wait = no user = root group = root groups = yes server = /usr/bin/clustercheck type = UNLISTED per_source = UNLIMITED log_on_success = log_on_failure = HOST flags = REUSE } Iniciar servicio: service xinetd enable service xinetd start NOTA: probablemente haya que cambiar el bind_address en los ficheros de configuraci\u00f3n de los servicios que corren en los controllers por la propia IP del controller. NOTA: para usar HAtop lanzar el siguiente comando: hatop -s /var/run/haproxy.sock","title":"0.2 - HaProxy"},{"location":"previos/00-2-HA - HAPROXY/#haproxy","text":"","title":"HAPROXY"},{"location":"previos/00-2-HA - HAPROXY/#1-instalacion","text":"Controllers apt install haproxy hatop","title":"1. Instalaci\u00f3n"},{"location":"previos/00-2-HA - HAPROXY/#2-configuracion","text":"","title":"2. Configuraci\u00f3n"},{"location":"previos/00-2-HA - HAPROXY/#21-rsyslog","text":"Para que HAProxy pueda logear eventos en un fichero local, tenemos que hacer un cambio en /etc/rsyslog.d/49-haproxy.conf . Deber\u00eda quedarnos as\u00ed: # Create an additional socket in haproxy's chroot in order to allow logging via # /dev/log to chroot'ed HAProxy processes #$AddUnixListenSocket /var/lib/haproxy/dev/log # # Send HAProxy messages to a dedicated logfile #if $programname startswith 'haproxy' then /var/log/haproxy.log #&~ # .. otherwise consider putting these two in /etc/rsyslog.conf instead: $ModLoad imudp $UDPServerAddress 127.0.0.1 $UDPServerRun 514 # ..and in any case, put these two in /etc/rsyslog.d/49-haproxy.conf: local0.* -/var/log/haproxy_0.log & ~ # & ~ means not to put what matched in the above line anywhere else for the rest of the rules # http://serverfault.com/questions/214312/how-to-keep-haproxy-log-messages-out-of-var-log-syslog Y reiniciamos el servicio: service rsyslog restart","title":"2.1. Rsyslog"},{"location":"previos/00-2-HA - HAPROXY/#22-haproxy","text":"IMPORTANTE: user haproxy en el chequeo de Galera, creado en la BBDD con un create user 'haproxy'@'172.16.100.%'; Hay que balancear todos los servicios, as\u00ed que el fichero /etc/haproxy/haproxy.cfg quedar\u00eda as\u00ed, m\u00e1s o menos: global log 127.0.0.1 local0 debug # log /var/lib/haproxy/dev/log local0 debug chroot /var/lib/haproxy daemon group haproxy maxconn 4000 pidfile /var/run/haproxy.pid user haproxy stats socket /var/run/haproxy.sock mode 600 level admin stats timeout 2m defaults log global maxconn 4000 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout check 10s listen rabbit_cluster # bind 172.16.100.10:5672 # balance source option tcpka # option httpchk option tcplog server controller01 172.16.100.11:5672 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:5672 check inter 2000 rise 2 fall 5 # server controller03p 192.168.111.13:5672 check inter 2000 rise 2 fall 5 listen dashboard_cluster bind 192.168.111.10:80 balance source option tcpka option httpchk option tcplog server controller01p 192.168.111.11:80 check inter 2000 rise 2 fall 5 server controller02p 192.168.111.12:80 check inter 2000 rise 2 fall 5 # server controller03p 192.168.111.13:80 check inter 2000 rise 2 fall 5 listen galera_cluster bind 192.168.111.10:3306 bind 172.16.100.10:3306 balance source option mysql-check user haproxy server controller01 172.16.100.11:3306 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:3306 backup check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:3306 backup check inter 2000 rise 2 fall 5 listen glance_api_cluster bind 172.16.100.10:9292 bind 192.168.111.10:9292 balance source option tcpka option httpchk option tcplog server controller01 172.16.100.11:9292 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:9292 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:9292 check inter 2000 rise 2 fall 5 listen glance_registry_cluster bind 172.16.100.10:9191 bind 192.168.111.10:9191 balance source option tcpka option tcplog server controller01 172.16.100.11:9191 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:9191 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:9191 check inter 2000 rise 2 fall 5 # listen keystone_admin_cluster bind 172.16.100.10:35357 balance source option tcpka option httpchk option tcplog server controller01 172.16.100.11:35357 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:35357 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:35357 check inter 2000 rise 2 fall 5 listen keystone_public_internal_cluster bind 192.168.111.10:5000 balance source option tcpka option httpchk option tcplog server controller01p 192.168.111.11:5000 check inter 2000 rise 2 fall 5 server controller02p 192.168.111.12:5000 check inter 2000 rise 2 fall 5 # server controller03p 192.168.111.13:5000 check inter 2000 rise 2 fall 5 listen keystone_internal_cluster bind 172.16.100.10:5000 balance source option tcpka option httpchk option tcplog server controller01 172.16.100.11:5000 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:5000 check inter 2000 rise 2 fall 5 # server controller03p 192.168.111.13:5000 check inter 2000 rise 2 fall 5 # listen nova_ec2_api_cluster # bind 172.16.100.10:8773 # balance source # option tcpka # option tcplog # server controller01 172.16.100.11:8773 check inter 2000 rise 2 fall 5 # server controller02 172.16.100.12:8773 check inter 2000 rise 2 fall 5 ## server controller03 172.16.100.13:8773 check inter 2000 rise 2 fall 5 listen nova_compute_api_cluster bind 172.16.100.10:8774 bind 192.168.111.10:8774 balance source option tcpka option httpchk option tcplog server controller01 172.16.100.11:8774 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:8774 check inter 2000 rise 2 fall 5 ## server controller03 172.16.100.13:8774 check inter 2000 rise 2 fall 5 # listen nova_metadata_api_cluster bind 172.16.100.10:8775 bind 192.168.111.10:8775 balance source option tcpka option tcplog server controller01 172.16.100.11:8775 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:8775 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:8775 check inter 2000 rise 2 fall 5 listen nova_placement_api_cluster bind 172.16.100.10:8778 bind 192.168.111.10:8778 balance source option tcpka option tcplog server controller01 172.16.100.11:8778 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:8778 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:8778 check inter 2000 rise 2 fall 5 listen cinder_api_cluster bind 172.16.100.10:8776 bind 192.168.111.10:8776 balance source option tcpka option httpchk option tcplog server controller01 172.16.100.11:8776 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:8776 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:8776 check inter 2000 rise 2 fall 5 listen heat_api_cfn_cluster bind 172.16.100.10:8000 bind 192.168.111.10:8000 balance source option tcpka option httpchk option tcplog server controller01 172.16.100.11:8000 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:8000 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:8000 check inter 2000 rise 2 fall 5 listen heat_api_cluster bind 172.16.100.10:8004 bind 192.168.111.10:8004 balance source option tcpka option httpchk option tcplog server controller01 172.16.100.11:8004 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:8004 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:8004 check inter 2000 rise 2 fall 5 listen gnocchi_api_cluster bind 172.16.100.10:8041 bind 192.168.111.10:8041 balance source option tcpka option httpchk option tcplog server controller01 172.16.100.11:8041 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:8041 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:8041 check inter 2000 rise 2 fall 5 # listen ceilometer_api_cluster # bind 172.16.100.10:8777 # balance source # option tcpka # option tcplog # server controller01 172.16.100.11:8777 check inter 2000 rise 2 fall 5 # server controller02 172.16.100.12:8777 check inter 2000 rise 2 fall 5 ## server controller03 172.16.100.13:8777 check inter 2000 rise 2 fall 5 listen nova_vncproxy_cluster bind 172.16.100.10:6080 bind 192.168.111.10:6080 balance source option tcpka option tcplog server controller01 172.16.100.11:6080 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:6080 check inter 2000 rise 2 fall 5 ## server controller03 172.16.100.13:6080 check inter 2000 rise 2 fall 5 # listen neutron_api_cluster bind 172.16.100.10:9696 bind 192.168.111.10:9696 balance source option tcpka option httpchk option tcplog server controller01 172.16.100.11:9696 check inter 2000 rise 2 fall 5 server controller02 172.16.100.12:9696 check inter 2000 rise 2 fall 5 # server controller03 172.16.100.13:9696 check inter 2000 rise 2 fall 5 Tambi\u00e9n hay que a\u00f1adir lo siguiente al fichero /etc/sysctl.conf para que los controllers puedan escuchar en interfaces de red externas (IP's virtuales): net.ipv4.ip_nonlocal_bind = 1 Y aplicar los cambios ejecutando sysctl -p EN UN CONTROLLER Despu\u00e9s hay que a\u00f1adir los recursos y restricciones a Pacemaker: pcs resource create lb-haproxy lsb:haproxy --clone pcs constraint order start vip then vip2 pcs constraint order start vip2 then lb-haproxy-clone pcs constraint colocation add vip with vip2 pcs constraint colocation add lb-haproxy-clone with vip pcs status Cluster name: openstackv1 Last updated: Wed Apr 18 10:37:00 2018 Last change: Wed Apr 18 10:26:51 2018 by root via cibadmin on controller01 Stack: corosync Current DC: controller02 (version 1.1.14-70404b0) - partition with quorum 2 nodes and 4 resources configured Online: [ controller01 controller02 ] Full list of resources: vip (ocf::heartbeat:IPaddr2): Started controller01 Clone Set: lb-haproxy-clone [lb-haproxy] Started: [ controller01 ] Stopped: [ controller02 ] vip2 (ocf::heartbeat:IPaddr2): Started controller01 PCSD Status: controller01: Online controller02: Online Daemon Status: corosync: active/enabled pacemaker: active/enabled pcsd: active/enabled","title":"2.2. HAProxy"},{"location":"previos/00-2-HA - HAPROXY/#3-comprobacion-adicional-para-galera","text":"Controller01 y Controller02 apt install xinetd Crear el fichero /etc/default/clusterchek : MYSQL_USERNAME=\"clustercheck_user\" MYSQL_PASSWORD=\"my_clustercheck_password\" MYSQL_HOST=\"localhost\" MYSQL_PORT=\"3306\" A\u00f1adir el usuario a la base de datos: GRANT PROCESS ON *.* TO 'clustercheck_user'@'localhost' IDENTIFIED BY 'my_clustercheck_password'; GRANT PROCESS ON *.* TO 'clustercheck_user'@'controller01' IDENTIFIED BY 'my_clustercheck_password'; GRANT PROCESS ON *.* TO 'clustercheck_user'@'controller02' IDENTIFIED BY 'my_clustercheck_password'; FLUSH PRIVILEGES; Crear el fichero /etc/xinetd.d/galera-monitor : service galera-monitor { port = 9200 disable = no socket_type = stream protocol = tcp wait = no user = root group = root groups = yes server = /usr/bin/clustercheck type = UNLISTED per_source = UNLIMITED log_on_success = log_on_failure = HOST flags = REUSE } Iniciar servicio: service xinetd enable service xinetd start NOTA: probablemente haya que cambiar el bind_address en los ficheros de configuraci\u00f3n de los servicios que corren en los controllers por la propia IP del controller. NOTA: para usar HAtop lanzar el siguiente comando: hatop -s /var/run/haproxy.sock","title":"3. Comprobaci\u00f3n adicional para Galera"},{"location":"previos/00-3-HA-RABBIT/","text":"Rabbit HA Crear un cluster de Rabbit Parar rabbit en el nodo de destino y copiar la cookie desde el primer nodo rsync -av /var/lib/rabbitmq/.erlang.cookie root@controller02:/var/lib/rabbitmq/.erlang.cookie En cada nodo de destino comprobar que .erlang.cookie tiene los permisos y propietario correctos: chown rabbitmq:rabbitmq /var/lib/rabbitmq/.erlang.cookie chmod 400 /var/lib/rabbitmq/.erlang.cookie Iniciamos el servicio de rabbit en la m\u00e1quina destino (controller02) y ejecutamos los siguientes comandos: service rabbitmq-server start rabbitmqctl cluster_status >Cluster status of node rabbit@controller02 ... >[{nodes,[{disc,[rabbit@controller02]}]}, > {running_nodes,[rabbit@controller02]}, > {cluster_name,<<\"rabbit@controller02\">>}, > {partitions,[]}] rabbitmqctl stop_app > Stopping node rabbit@controller02 ... rabbitmqctl join_cluster rabbit@controller01 >Clustering node rabbit@controller02 with rabbit@controller01 ... >>> **NOTA:** Podemos cambiar el tipo de nodo si en vez de `--ram` ponemos `--disc` rabbitmqctl start_app >Starting node rabbit@controller02 ... comprobamos que se ha unido al cluster (controller01 o 02 da lo mismo): rabbitmqctl cluster_status >Cluster status of node rabbit@controller01 ... >[{nodes,[{disc,[rabbit@controller01,rabbit@controller02]}]}, > {running_nodes,[rabbit@controller02,rabbit@controller01]}, > {cluster_name,<<\"rabbit@controller02\">>}, > {partitions,[]}] Para asegurarnos de que todas las colas excepto aquellas con nombres autogenerados se repliquen en todos los nodos, establecemos la pol\u00edtica ha-mode ejecutando lo siguiente en uno de los nodos: rabbitmqctl set_policy ha-all '^(?!amq\\.).*' '{\"ha-mode\": \"all\"}' Configurar componentes de Openstack En los ficheros de configuraci\u00f3n de los componentes de Openstack, a\u00f1adir lo siguiente: transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack rabbit_retry_interval=1 rabbit_retry_backoff=2 rabbit_max_retries=0 rabbit_durable_queues=true rabbit_ha_queues=true Y reiniciar los componentes Controller01 y Controller02: /etc/cinder/cinder.conf /etc/heat/heat.conf /etc/ceilometer/ceilometer.conf /etc/nova/nova.conf /etc/neutron/neutron.conf /etc/glance/glance-api.conf /etc/glance/glance-registry.conf KVM01 y KVM02: /etc/nova/nova.conf /etc/ceilometer/ceilometer.conf /etc/neutron/neutron.conf Gateway: /etc/neutron/neutron.conf Storage: /etc/cinder/cinder.conf","title":"0.3 - RabbitMQ"},{"location":"previos/00-3-HA-RABBIT/#rabbit-ha","text":"","title":"Rabbit HA"},{"location":"previos/00-3-HA-RABBIT/#crear-un-cluster-de-rabbit","text":"Parar rabbit en el nodo de destino y copiar la cookie desde el primer nodo rsync -av /var/lib/rabbitmq/.erlang.cookie root@controller02:/var/lib/rabbitmq/.erlang.cookie En cada nodo de destino comprobar que .erlang.cookie tiene los permisos y propietario correctos: chown rabbitmq:rabbitmq /var/lib/rabbitmq/.erlang.cookie chmod 400 /var/lib/rabbitmq/.erlang.cookie Iniciamos el servicio de rabbit en la m\u00e1quina destino (controller02) y ejecutamos los siguientes comandos: service rabbitmq-server start rabbitmqctl cluster_status >Cluster status of node rabbit@controller02 ... >[{nodes,[{disc,[rabbit@controller02]}]}, > {running_nodes,[rabbit@controller02]}, > {cluster_name,<<\"rabbit@controller02\">>}, > {partitions,[]}] rabbitmqctl stop_app > Stopping node rabbit@controller02 ... rabbitmqctl join_cluster rabbit@controller01 >Clustering node rabbit@controller02 with rabbit@controller01 ... >>> **NOTA:** Podemos cambiar el tipo de nodo si en vez de `--ram` ponemos `--disc` rabbitmqctl start_app >Starting node rabbit@controller02 ... comprobamos que se ha unido al cluster (controller01 o 02 da lo mismo): rabbitmqctl cluster_status >Cluster status of node rabbit@controller01 ... >[{nodes,[{disc,[rabbit@controller01,rabbit@controller02]}]}, > {running_nodes,[rabbit@controller02,rabbit@controller01]}, > {cluster_name,<<\"rabbit@controller02\">>}, > {partitions,[]}] Para asegurarnos de que todas las colas excepto aquellas con nombres autogenerados se repliquen en todos los nodos, establecemos la pol\u00edtica ha-mode ejecutando lo siguiente en uno de los nodos: rabbitmqctl set_policy ha-all '^(?!amq\\.).*' '{\"ha-mode\": \"all\"}'","title":"Crear un cluster de Rabbit"},{"location":"previos/00-3-HA-RABBIT/#configurar-componentes-de-openstack","text":"En los ficheros de configuraci\u00f3n de los componentes de Openstack, a\u00f1adir lo siguiente: transport_url = rabbit://openstackuser:openstackpass@controller01:5672,openstackuser:openstackpass@controller02:5672/openstack rabbit_retry_interval=1 rabbit_retry_backoff=2 rabbit_max_retries=0 rabbit_durable_queues=true rabbit_ha_queues=true Y reiniciar los componentes Controller01 y Controller02: /etc/cinder/cinder.conf /etc/heat/heat.conf /etc/ceilometer/ceilometer.conf /etc/nova/nova.conf /etc/neutron/neutron.conf /etc/glance/glance-api.conf /etc/glance/glance-registry.conf KVM01 y KVM02: /etc/nova/nova.conf /etc/ceilometer/ceilometer.conf /etc/neutron/neutron.conf Gateway: /etc/neutron/neutron.conf Storage: /etc/cinder/cinder.conf","title":"Configurar componentes de Openstack"},{"location":"previos/00-4-HA-MariaDB-Galera/","text":"MariaDB en HA - Galera 1. Instalaci\u00f3n En todos los controllers A\u00f1adimos el repositorio e instalamos mariadb: apt-key adv --recv-keys --keyserver hkp://keyserver.ubuntu.com:80 0xF1656F24C74CD1D8 add-apt-repository 'deb [arch=amd64,i386,ppc64el] http://nyc2.mirrors.digitalocean.com/mariadb/repo/10.1/ubuntu xenial main' apt update apt install rsync mariadb-server python-pymysql NOTA: tenemos que asegurarnos de poner contrase\u00f1a al usuario root de la bbdd para no tener que lidiar con el unix_socket etc. Creamos el fichero /etc/mysql/conf.d/galera.cnf (o en /etc/mysql/mariadb.conf.d/50-server.cnf ): [mysqld] datadir=/var/lib/mysql #socket=/var/lib/mysql/mysql.sock user=mysql binlog_format=ROW bind-address=172.16.100.1x skip-external-locking plugin_load=server_audit=server_audit.so #plugin-load-add = auth_socket.so character-set-server = utf8 collation-server = utf8_general_ci max_connections = 2000 # InnoDB Configuration default_storage_engine=innodb innodb_autoinc_lock_mode=2 innodb_flush_log_at_trx_commit=0 innodb_buffer_pool_size=122M # Galera Provider Configuration wsrep_on=ON wsrep_provider=/usr/lib/galera/libgalera_smm.so # Galera Cluster Configuration wsrep_provider_options=\"pc.recovery=TRUE;gcache.size=300M\" wsrep_cluster_name=\"openstack_cluster\" wsrep_cluster_address=\"gcomm://172.16.100.11:4567,172.16.100.12:4567,172.16.100.10:4567\" ##podemos probar a usar nombers de host # Galera Synchronization Configuration wsrep_sst_method=rsync # Galera Node Configuration wsrep_node_address=\"172.16.100.1x\" wsrep_node_name=\"controller0x\" # * Logging and Replication general_log_file = /var/log/mysql/mysql.log general_log = 1 # # Error log - should be very few entries. # log_error = /var/log/mysql/error.log log-warnings = 2 # #slow_query_log_file = /var/log/mysql/mariadb-slow.log #long_query_time = 10 #log_slow_rate_limit = 1000 #log_slow_verbosity = query_plan #log-queries-not-using-indexes NOTA: cambiamos los par\u00e1metros bind-address , wsrep_cluster_address y wsrep_node_name acorde con el nodo que estamos configurando. paramos el servicio en los controllers service mysql stop Inicializamos el primer nodo (controller01) y comprobamos que todo est\u00e1 bien: galera_new_cluster mysql -u root -p -e \"SHOW STATUS LIKE 'wsrep_cluster_size'\" +--------------------+-------+ | Variable_name | Value | +--------------------+-------+ | wsrep_cluster_size | 1 | +--------------------+-------+ Iniciamos el resto de nodos con y comprobamos: service mysql start mysql -u root -p -e \"SHOW STATUS LIKE 'wsrep_cluster_size'\" +--------------------+-------+ | Variable_name | Value | +--------------------+-------+ | wsrep_cluster_size | 2 | +--------------------+-------+ \u00c1rbitro Instalamos el \u00e1rbitro en el balanceador: apt-key adv --recv-keys --keyserver hkp://keyserver.ubuntu.com:80 0xF1656F24C74CD1D8 add-apt-repository 'deb [arch=amd64,i386,ppc64el] http://nyc2.mirrors.digitalocean.com/mariadb/repo/10.1/ubuntu xenial main' apt update apt install galera-arbitrator-3 y a\u00f1adir esto al fichero /etc/default/garb : # Copyright (C) 2012 Codership Oy # This config file is to be sourced by garb service script. # A comma-separated list of node addresses (address[:port]) in the cluster GALERA_NODES=\"172.16.100.11:4567, 172.16.100.12:4567\" # Galera cluster name, should be the same as on the rest of the nodes. GALERA_GROUP=\"openstack_cluster\" # Optional Galera internal options string (e.g. SSL settings) # see http://galeracluster.com/documentation-webpages/galeraparameters.html # GALERA_OPTIONS=\"\" # Log file for garbd. Optional, by default logs to syslog LOG_FILE=\"/var/log/garbd.log\" NOTA: para ejecutar un \u00e1rbitro y que \u00e9ste arranque hay que hacer lo siguiente: touch /var/log/garbd.log && chown nobody /var/log/garbd.log Arrancamos el servicio de \u00e1rbitro: systemctl enable garb service garb start Comprobamos que tenemos 3 miembros en el cl\u00faster, desde cualquier controller (controller01): mysql -u root -p -e \"SHOW STATUS LIKE 'wsrep_cluster_size'\" +--------------------+-------+ | Variable_name | Value | +--------------------+-------+ | wsrep_cluster_size | 3 | +--------------------+-------+ Creamos el usuario root para el resto de controllers (controller02,03...): mysql -u root -p create user 'root'@'controller0X' IDENTIFIED BY 'password'; GRANT ALL PRIVILEGES ON *.* TO 'root'@'controller0X' IDENTIFIED BY 'password' WITH GRANT OPTION; FLUSH PRIVILEGES; DEBUGGING y EXTRAS Para crear usuarios en la BBDD hay que usar SIEMPRE create user XXX y no INSERT INTO ... SET GLOBAL server_audit_logging=OFF; SET GLOBAL server_audit_events = 'CONNECT'; Para recuperar un cluster que no arranca, ejecutar esto en cada nodo y luego cambiar el valor safe_to_bootstrap del fichero /var/lib/mysql/grastate.dat a 1 del nodo con el n\u00famero de secuencia m\u00e1s alto. En caso de que sea -1, elegir el que m\u00e1s rabia nos d\u00e9: mysqld_safe --wsrep-recover","title":"0.4 - MariaDB - Galera cluster"},{"location":"previos/00-4-HA-MariaDB-Galera/#mariadb-en-ha-galera","text":"","title":"MariaDB en HA - Galera"},{"location":"previos/00-4-HA-MariaDB-Galera/#1-instalacion","text":"En todos los controllers A\u00f1adimos el repositorio e instalamos mariadb: apt-key adv --recv-keys --keyserver hkp://keyserver.ubuntu.com:80 0xF1656F24C74CD1D8 add-apt-repository 'deb [arch=amd64,i386,ppc64el] http://nyc2.mirrors.digitalocean.com/mariadb/repo/10.1/ubuntu xenial main' apt update apt install rsync mariadb-server python-pymysql NOTA: tenemos que asegurarnos de poner contrase\u00f1a al usuario root de la bbdd para no tener que lidiar con el unix_socket etc. Creamos el fichero /etc/mysql/conf.d/galera.cnf (o en /etc/mysql/mariadb.conf.d/50-server.cnf ): [mysqld] datadir=/var/lib/mysql #socket=/var/lib/mysql/mysql.sock user=mysql binlog_format=ROW bind-address=172.16.100.1x skip-external-locking plugin_load=server_audit=server_audit.so #plugin-load-add = auth_socket.so character-set-server = utf8 collation-server = utf8_general_ci max_connections = 2000 # InnoDB Configuration default_storage_engine=innodb innodb_autoinc_lock_mode=2 innodb_flush_log_at_trx_commit=0 innodb_buffer_pool_size=122M # Galera Provider Configuration wsrep_on=ON wsrep_provider=/usr/lib/galera/libgalera_smm.so # Galera Cluster Configuration wsrep_provider_options=\"pc.recovery=TRUE;gcache.size=300M\" wsrep_cluster_name=\"openstack_cluster\" wsrep_cluster_address=\"gcomm://172.16.100.11:4567,172.16.100.12:4567,172.16.100.10:4567\" ##podemos probar a usar nombers de host # Galera Synchronization Configuration wsrep_sst_method=rsync # Galera Node Configuration wsrep_node_address=\"172.16.100.1x\" wsrep_node_name=\"controller0x\" # * Logging and Replication general_log_file = /var/log/mysql/mysql.log general_log = 1 # # Error log - should be very few entries. # log_error = /var/log/mysql/error.log log-warnings = 2 # #slow_query_log_file = /var/log/mysql/mariadb-slow.log #long_query_time = 10 #log_slow_rate_limit = 1000 #log_slow_verbosity = query_plan #log-queries-not-using-indexes NOTA: cambiamos los par\u00e1metros bind-address , wsrep_cluster_address y wsrep_node_name acorde con el nodo que estamos configurando. paramos el servicio en los controllers service mysql stop Inicializamos el primer nodo (controller01) y comprobamos que todo est\u00e1 bien: galera_new_cluster mysql -u root -p -e \"SHOW STATUS LIKE 'wsrep_cluster_size'\" +--------------------+-------+ | Variable_name | Value | +--------------------+-------+ | wsrep_cluster_size | 1 | +--------------------+-------+ Iniciamos el resto de nodos con y comprobamos: service mysql start mysql -u root -p -e \"SHOW STATUS LIKE 'wsrep_cluster_size'\" +--------------------+-------+ | Variable_name | Value | +--------------------+-------+ | wsrep_cluster_size | 2 | +--------------------+-------+","title":"1. Instalaci\u00f3n"},{"location":"previos/00-4-HA-MariaDB-Galera/#arbitro","text":"Instalamos el \u00e1rbitro en el balanceador: apt-key adv --recv-keys --keyserver hkp://keyserver.ubuntu.com:80 0xF1656F24C74CD1D8 add-apt-repository 'deb [arch=amd64,i386,ppc64el] http://nyc2.mirrors.digitalocean.com/mariadb/repo/10.1/ubuntu xenial main' apt update apt install galera-arbitrator-3 y a\u00f1adir esto al fichero /etc/default/garb : # Copyright (C) 2012 Codership Oy # This config file is to be sourced by garb service script. # A comma-separated list of node addresses (address[:port]) in the cluster GALERA_NODES=\"172.16.100.11:4567, 172.16.100.12:4567\" # Galera cluster name, should be the same as on the rest of the nodes. GALERA_GROUP=\"openstack_cluster\" # Optional Galera internal options string (e.g. SSL settings) # see http://galeracluster.com/documentation-webpages/galeraparameters.html # GALERA_OPTIONS=\"\" # Log file for garbd. Optional, by default logs to syslog LOG_FILE=\"/var/log/garbd.log\" NOTA: para ejecutar un \u00e1rbitro y que \u00e9ste arranque hay que hacer lo siguiente: touch /var/log/garbd.log && chown nobody /var/log/garbd.log Arrancamos el servicio de \u00e1rbitro: systemctl enable garb service garb start Comprobamos que tenemos 3 miembros en el cl\u00faster, desde cualquier controller (controller01): mysql -u root -p -e \"SHOW STATUS LIKE 'wsrep_cluster_size'\" +--------------------+-------+ | Variable_name | Value | +--------------------+-------+ | wsrep_cluster_size | 3 | +--------------------+-------+ Creamos el usuario root para el resto de controllers (controller02,03...): mysql -u root -p create user 'root'@'controller0X' IDENTIFIED BY 'password'; GRANT ALL PRIVILEGES ON *.* TO 'root'@'controller0X' IDENTIFIED BY 'password' WITH GRANT OPTION; FLUSH PRIVILEGES;","title":"\u00c1rbitro"},{"location":"previos/00-4-HA-MariaDB-Galera/#debugging-y-extras","text":"Para crear usuarios en la BBDD hay que usar SIEMPRE create user XXX y no INSERT INTO ... SET GLOBAL server_audit_logging=OFF; SET GLOBAL server_audit_events = 'CONNECT'; Para recuperar un cluster que no arranca, ejecutar esto en cada nodo y luego cambiar el valor safe_to_bootstrap del fichero /var/lib/mysql/grastate.dat a 1 del nodo con el n\u00famero de secuencia m\u00e1s alto. En caso de que sea -1, elegir el que m\u00e1s rabia nos d\u00e9: mysqld_safe --wsrep-recover","title":"DEBUGGING y EXTRAS"},{"location":"previos/previos/","text":"0. Previos 0.0 ACHTUNG! Es recomendable a\u00f1adir todos los hosts de OpenStack al fichero /etc/hosts Desde los nodos de computaci\u00f3n las referencias a la IP externa del controller no tienen sentido excepto cuando referencian a la consola VNC Nos aseguramos de a\u00f1adir lo siguiente al fichero /etc/hosts : 172.16.100.11 controller01 192.168.111.11 controller01p 172.16.100.12 controller02 192.168.111.12 controller02p 172.16.100.51 gateway01 172.16.100.52 gateway02 172.16.100.101 kvm01 172.16.100.102 kvm02 172.16.100.151 storage01 ## IP's virtuales 172.16.100.10 controller 192.168.111.10 controllerp Controller01 y 02 Instalamos memcached : apt install memcached A la hora de configurar memcached, en los controllers lo haremos escuchar por la interfaz de management. En /etc/memcached.conf : -l controller0x Comprobamos que est\u00e1 escuchando en la interfaz deseada: netstat -plutn | grep memcache NOTA: Y en los ficheros de configuraci\u00f3n de los servicios apuntaremos a las direcciones de todos los controllers en el apartado [cache] . [cache] enabled = true backend = dogpile.cache.memcached memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3 0.0.5 SSH SIN CONTRASE\u00d1A PARA ROOT hacemos lo siguiente en ambos controllers como root: ssh-keygen scp /root/.ssh/id_rsa.pub hvsistemas@controller0x:/tmp/ cat /tmp/id_rsa.pub > /root/.ssh/authorized_keys 0.1 Actualizaci\u00f3n de paquetes controller01, controller02, gateway01, gateway02, storage01, kvm01 y kvm02 sudo apt update sudo apt dist-upgrade sudo reboot 0.2 NTP controller01, controller02, gateway01, gateway02, storage01, kvm01 y kvm02 Instalamos el cliente ntp: sudo apt install ntp ntpdate 0.3 Repositorios OpenStack Pike controller01, controller02, gateway01, gateway02, storage01, kvm01 y kvm02 A\u00f1adimos el repositorio para pike y actualizamos los paquetes: sudo apt install software-properties-common sudo add-apt-repository cloud-archive:pike sudo apt update && sudo apt dist-upgrade 0.4 RabbitMQ controller01 y controller02 Instalamos el broker RabbitMQ: sudo apt install rabbitmq-server Modificamos el fichero de configuraci\u00f3n /etc/rabbitmq/rabbitmq-env.conf : NODE_IP_ADDRESS=controller0x En un controller (el que vayamos a usar como referencia para el HA) creamos el user y vhost y establecemos permisos: sudo rabbitmqctl add_user openstackuser openstackpass sudo rabbitmqctl add_vhost openstack sudo rabbitmqctl set_permissions -p openstack openstackuser \".*\" \".*\" \".*\" Reiniciamos el servicio: sudo service rabbitmq-server restart Seguir en 00-3-HA-RABBIT.md si queremos alta disponibilidad 0.5 MariaDB > para HA Mirar 00-4-HA - MariaDB - Galera.md controller01 Instalamos MariaDB: sudo apt install mariadb-server python-pymysql Editamos el fichero /etc/mysql/mariadb.conf.d/50-server.cnf : bind-address = controller0x character-set-server = utf8 collation-server = utf8_general_ci Reiniciamos el servicio: sudo service mysql restart 0.6 API controller01 controller02 Instalamos los clientes de las APIs: sudo apt install python-openstackclient Ejecutamos el siguiente comando para tener bash completion: openstack complete | sudo tee /etc/bash_completion.d/osc.bash_completion > /dev/null Extra extra comandos \u00fatiles: openstack usage show --project hispavista --start 2018-01-05 --end 2018-01-06 openstack usage list --start 2018-01-05 --end 2018-01-06 Alta disponibilidad Gu\u00eda oficial","title":"0.0 - Configuraci\u00f3n inicial"},{"location":"previos/previos/#0-previos","text":"","title":"0. Previos"},{"location":"previos/previos/#00-achtung","text":"Es recomendable a\u00f1adir todos los hosts de OpenStack al fichero /etc/hosts Desde los nodos de computaci\u00f3n las referencias a la IP externa del controller no tienen sentido excepto cuando referencian a la consola VNC Nos aseguramos de a\u00f1adir lo siguiente al fichero /etc/hosts : 172.16.100.11 controller01 192.168.111.11 controller01p 172.16.100.12 controller02 192.168.111.12 controller02p 172.16.100.51 gateway01 172.16.100.52 gateway02 172.16.100.101 kvm01 172.16.100.102 kvm02 172.16.100.151 storage01 ## IP's virtuales 172.16.100.10 controller 192.168.111.10 controllerp Controller01 y 02 Instalamos memcached : apt install memcached","title":"0.0 ACHTUNG!"},{"location":"previos/previos/#a-la-hora-de-configurar-memcached-en-los-controllers-lo-haremos-escuchar-por-la-interfaz-de-management-en-etcmemcachedconf","text":"-l controller0x Comprobamos que est\u00e1 escuchando en la interfaz deseada: netstat -plutn | grep memcache NOTA: Y en los ficheros de configuraci\u00f3n de los servicios apuntaremos a las direcciones de todos los controllers en el apartado [cache] . [cache] enabled = true backend = dogpile.cache.memcached memcache_servers = controller01:11211, controller02:11211 memcache_dead_retry = 300 memcache_socket_timeout = 3","title":"A la hora de configurar memcached, en los controllers lo haremos escuchar por la interfaz de management. En /etc/memcached.conf:"},{"location":"previos/previos/#005-ssh-sin-contrasena-para-root","text":"hacemos lo siguiente en ambos controllers como root: ssh-keygen scp /root/.ssh/id_rsa.pub hvsistemas@controller0x:/tmp/ cat /tmp/id_rsa.pub > /root/.ssh/authorized_keys","title":"0.0.5 SSH SIN CONTRASE\u00d1A PARA ROOT"},{"location":"previos/previos/#01-actualizacion-de-paquetes","text":"controller01, controller02, gateway01, gateway02, storage01, kvm01 y kvm02 sudo apt update sudo apt dist-upgrade sudo reboot","title":"0.1 Actualizaci\u00f3n de paquetes"},{"location":"previos/previos/#02-ntp","text":"controller01, controller02, gateway01, gateway02, storage01, kvm01 y kvm02 Instalamos el cliente ntp: sudo apt install ntp ntpdate","title":"0.2 NTP"},{"location":"previos/previos/#03-repositorios-openstack-pike","text":"controller01, controller02, gateway01, gateway02, storage01, kvm01 y kvm02 A\u00f1adimos el repositorio para pike y actualizamos los paquetes: sudo apt install software-properties-common sudo add-apt-repository cloud-archive:pike sudo apt update && sudo apt dist-upgrade","title":"0.3 Repositorios OpenStack Pike"},{"location":"previos/previos/#04-rabbitmq","text":"controller01 y controller02 Instalamos el broker RabbitMQ: sudo apt install rabbitmq-server Modificamos el fichero de configuraci\u00f3n /etc/rabbitmq/rabbitmq-env.conf : NODE_IP_ADDRESS=controller0x En un controller (el que vayamos a usar como referencia para el HA) creamos el user y vhost y establecemos permisos: sudo rabbitmqctl add_user openstackuser openstackpass sudo rabbitmqctl add_vhost openstack sudo rabbitmqctl set_permissions -p openstack openstackuser \".*\" \".*\" \".*\" Reiniciamos el servicio: sudo service rabbitmq-server restart Seguir en 00-3-HA-RABBIT.md si queremos alta disponibilidad","title":"0.4 RabbitMQ"},{"location":"previos/previos/#05-mariadb-para-ha-mirar-00-4-ha-mariadb-galeramd","text":"controller01 Instalamos MariaDB: sudo apt install mariadb-server python-pymysql Editamos el fichero /etc/mysql/mariadb.conf.d/50-server.cnf : bind-address = controller0x character-set-server = utf8 collation-server = utf8_general_ci Reiniciamos el servicio: sudo service mysql restart","title":"0.5 MariaDB &gt; para HA Mirar 00-4-HA - MariaDB - Galera.md"},{"location":"previos/previos/#06-api","text":"controller01 controller02 Instalamos los clientes de las APIs: sudo apt install python-openstackclient Ejecutamos el siguiente comando para tener bash completion: openstack complete | sudo tee /etc/bash_completion.d/osc.bash_completion > /dev/null","title":"0.6 API"},{"location":"previos/previos/#extra-extra","text":"comandos \u00fatiles: openstack usage show --project hispavista --start 2018-01-05 --end 2018-01-06 openstack usage list --start 2018-01-05 --end 2018-01-06 Alta disponibilidad Gu\u00eda oficial","title":"Extra extra"}]}